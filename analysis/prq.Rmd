---
title: "Puppy Raiser Questionnaire vs Outcome (Success, Fail)"
author: "Marinara Marcato"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/marinara.marcato/Project/Scripts/dog_questionnaires")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
#install.packages("")
library(DescTools)
library(ggplot2)
library(cowplot)
library(plyr)
library(dplyr)
library(tidyverse)
library(rstatix)
library(ggpubr)
library(ggsci) # colors for the graphs - npg for nature


# feature selection using recursive feature elimination (RFE)
library(mlbench)
library(caret)
library(reshape2)
library(recipes)
```

# Introduction 
This document tests assess the association between behaviours reported in the standardised questionnaire Canine Behavioural Assessement & Research Questionnaire (C-BARQ) and the dog's training outcome.

# Data Exploration

Importing data and converting to adequate data types.
```{r, echo = FALSE}
prq = read.csv('data//2022-05-10-PRQ_C-BARQ.csv', stringsAsFactors=TRUE)
# colnames(prq)
# converting date types
prq$Timestamp = as.Date(prq$Timestamp, format= "%Y-%m-%d")
prq$DOB = as.Date(prq$DOB, format= "%Y-%m-%d")
prq$DOA = as.Date(prq$DOA, format= "%Y-%m-%d")
prq$End.Date = as.Date(prq$End.Date, format= "%Y-%m-%d")
dtq$Duration = as.numeric(gsub(" .*$", "", dtq$Duration))
```

## Demographics

The number of dogs in the Puppy Raiser Questionnaire dataset. Selection criteria: dogs who successed training (assistance and guide dogs) and dogs withdrawn for behavioural reasons.
```{r, echo = FALSE}
cat('Number of dogs:', length(prq$Code))
cat('Training Outcome:')
table(prq$Outcome)
```

Analysing categorical demographic data: Sex, Breed. There was only one German Shepherd and Golden Doodle in the sample, their breeds were relabeled as "Other" for the data analysis.
```{r, echo = FALSE}
print(table(prq$Sex))
print("Original classes")
table(prq$Breed)
# merging breed categories
levels(prq$Breed)[levels(prq$Breed) == "LRx"] <- "LRxGR"
levels(prq$Breed)[levels(prq$Breed) =="GS" | levels(prq$Breed) =="GRxPoodle"] <- "Other"
print("Processed classes")
table(prq$Breed)
```

Calculate descriptive statistics for age at assessment:
```{r, echo = FALSE}
n <- dim(prq)[1]

prq$Age.at.Arrival <- prq$DOA - prq$DOB
mean <- mean(prq$Age.at.Arrival)
std <- sd(prq$Age.at.Arrival)
margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)

cat('Age at Arrival: Mean', round(mean/30.417, 2), 
            'Standard Deviation', round(std/30.417, 2))
            # 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)


prq$Age.at.Assessment <- prq$Timestamp - prq$DOB
mean <- mean(prq$Age.at.Assessment)
std <- sd(prq$Age.at.Assessment)
margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)

cat('Age at Assessment: Mean', round(mean/30.417, 2), 
            'Standard Deviation', round(std/30.417, 2))
            # 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)

# Duration of training before withdrawal in weeks
duration <- prq %>% filter(Outcome == "Fail") %>% pull(Duration)/7
summary(duration)
hist(duration)

```

## Descriptive Statistics
The C-BARQ contains 100 items which were scored by the Puppy Raisers in a scale from 'Never', 'Seldom', 'Sometimes', 'Usually', 'Always' and encoded in a scale from 0 to 4. 
Out of the 100 items, 78 items are used to calculate 14 factors and the remaining 22 items are myscellaneous.
The questionnaire data are kept as numeric, rather than being converted to factors, so the information about the ordering is kept.
The descriptive statistics of the questionnaire data shown below are calculated and saved as a csv.  
Missing data is also analysed. 

```{r, echo = FALSE}
data = prq %>% select(starts_with(c('X', 'F_')), Sex, Breed, Outcome)
# LR will predict the probability of the LAST class 
# R defaults to alphabetical order, therefore, between [Fail Success], it will predict Success.
# releveling the independent variable so we will predict Fail instead of Success
data$Outcome <- relevel(data$Outcome, "Success")
print("Items")
colnames(data[1:100])
print("Factors, Demographic and Outcome")
str(data[101:117])
# FACTOR CALCULATIONS
    # 1.'Stranger-directed aggression' score = (questionnaire items 10 + 11 + 12 + 15 + 16 + 18 + 20 + 21 + 22 + 28)/10.
    # 2.'Owner-directed aggression' score = (items 9 + 13 + 14 + 17 + 19 + 25 + 30 + 31)/8.
    # 3.'Dog-directed aggression' = (items 23 + 24 + 26 + 29)/4
    # 4.'Dog-directed fear' = (items 45 + 46 + 52 + 53)/4.
        # Item 33 approached in 
    # 5.'Dog rivalry'(familiar dog aggression) score = (items 32 + 33 + 34 + 35)/4
    # 6.'Trainability' score = (items 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8)/8—remember to reverse scoring order for items 5, 6 & 7 (see above).
        
    # 7.'Chasing' score = (items 27 + 74 + 75 + 76)/4
    # 8.'Stranger-directed fear' score = (items 36 + 37 + 39 + 40)/4
    # 9.'Nonsocial fear' score = (items 38 + 41 + 42 + 44 + 47 + 48)/6
    # 10.'Separation-related problems' score = (items 54 + 55 + 56 + 57 + 58 + 59 + 60 + 61)/8

    # 11.'Touch sensitivity' score = (items 43 + 49 + 50 + 51)/4
    # 12.'Excitability' score = (items 62 + 63 + 64 + 65 + 66 + 67)/6
    # 13.'Attachment/attention-seeking' score = (items  68 + 69 + 70 + 71 + 72 + 73)/6
    # 14.'Energy' score = (items 91 + 92)/2

    # Factors use 1–76 and 91–92
    # Factors do NOT use item 77-90 and 93-100 
```

```{r, echo = FALSE}
# calculate descriptive statistics
stats <- data.frame(do.call(rbind, lapply(data[1:114], summary)))

# missing values, there is something weird about the NAs calculations using summary
stats <- cbind(stats, colSums(is.na(data[1:114])))
colnames(stats)[8] <- "Missing"
hist(stats$Missing, breaks = 20)
print('Features sorted by number of missing rows')
print(stats[order(stats$Missing, decreasing = TRUE)[1:15], 'Missing', drop = FALSE])

# 28 NAs -> Item 49 nails clipped by someone in the house
# 12-10 NAs -> Because of no other dog  in household -> Items 33,34,32,35

# save descriptive statistics of the dataset to csv
write.csv(stats, "models/prq-stats.csv") 
```

## Univariate Logistic Regression
Logistic Regression one-way analysis of variance test was used to investigate whether there was a statistically significant difference between the behaviours reported by Puppy Raisers and the Training Outcome of the dogs.
C-BARQ Correlation with Training Outcome. 

Breeds (p = 0.44 and p = 0.16) and Sex (p = 0.61) were not significantly associated with training outcome.
```{r, echo = FALSE}
# univariate logistic regression using one predictor and outcome
lr_models <- lapply(data[-length(data)], function(x) summary(glm(formula = Outcome ~ x, 
                            data = data, family = binomial(link = logit), na.action = na.exclude)))
lr_result <- lapply(lr_models, function(x) c(coef(x), x$deviance))
lr_models[115]
## univariate models with 3 dfs (Breed)
lr_breeds <- data.frame(transpose(lr_result[116]))
colnames(lr_breeds) <- c("estimate_0", "estimate_1", "estimate_2", 
                    "se_0", "se_1", "se_2", 
                    "z_value_0", "z_value_1", "z_value_2", 
                    "p_value_0", "p_value_1", "p_value_2", "deviance")
print(unique(data$Breed))
cat('Difference between Other and LR: \t', lr_breeds$p_value_1, '\t -> not significant')
cat('Difference between Other and LRxGR: \t', lr_breeds$p_value_2, '\t -> not significant')

## univariate models with up to 2 dfs (items are ints)
lr_results <- data.frame(do.call(rbind,lr_result[1:115]))
colnames(lr_results) <- c("estimate_0", "estimate_1", "se_0", "se_1", "z_value_0", "z_value_1",
                    "p_value_0", "p_value_1", "deviance")

# check if the columns were named appropriately 
# lr_models[1]
# lr_results[1,]

## univariate models with 1 df (because the predictor variable is constant)
print("Columns 9,10,13,17,30,37 are CONSTANT, that's why they p_values don't make sense")
lr_results[c(9,10,13,17,30,37),'p_value_1']
```

Analysing p-values.
```{r, echo = FALSE}
# print variables sorted by pvalue
lr_results[order(lr_results$p_value_1, decreasing = FALSE), 'p_value_1' , drop = FALSE]
# pvalue analysis
lr_results$significant_0.05 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.05)
lr_results$significant_0.1 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.1)
lr_results$significant_0.2 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.2)
#  lr_results[order(lr_results$p_value_1),]
print("Number of features considering each level of significance using Logistic Regression p-value test")
print(colSums(lr_results[,10:12], na.rm = TRUE))
```

Analysing deviance of full model compared to the null model. 
Reduction in deviance must be significant (chi-squared test) for the model to be considered a good fit.
```{r, echo = FALSE}
# deviance analysis (Null deviance: 58.352  on 62  degrees of freedom = N-1)
lr_results$deviance_diff <- 1 - pchisq((58.352 - lr_results$deviance), 1, lower.tail = FALSE)
lr_results$significant_dev <- (lr_results$deviance_diff > 0.95)
# significant differences in deviances
cat('Number of features whose univariate models resulted in a significant difference in deviance: ', sum(lr_results$significant_dev))
```

Data selection based on the univariate logistic regression p-value and deviance analysis. 
Full feature set constains all factors and all items significant p < 0.05.
Reduced feature set contains all factors and items NOT included in the calculation of ANY factor with p<0.05 (i.e. Items 77-90, 93-100).

```{r, echo = FALSE}
# all of the features with p < 0.2 had significant reduction in deviance too, the filtering by two conditions is NOT necessary in that case
# lr_results %>% filter(p_value_1 <0.2 & significant_dev == TRUE) %>%  select(p_value_1, significant_dev)
# selecting features with p < 0.2
data_0.2_full <- data %>% select(all_of(rownames(lr_results[lr_results$p_value_1 < 0.2, ])), Outcome)
data_0.2_reduced <- data_0.2_full %>% select(contains(c(as.character(c(77:90, 93:100)), 'Sex', 'Breed', 'F_')), Outcome)
# inclusion of X96,98 and F3,6 compared to feature set with 0.05 
# colnames(data_0.2_reduced)
# dim(data_0.2_reduced)

# print("All features with level of significance < 0.05 using Univariate Logistic Regression p-value test")
# print(lr_results[lr_results$significant_0.05 == TRUE, 'p_value_1', drop = FALSE])

# selecting features with p < 0.05 
data_0.05_full <- data %>% select(all_of(rownames(lr_results[lr_results$significant_0.05 ,])), Outcome)
cat("Full Feature Set with p<0.05\t", dim(data_0.05_full))
colnames(data_0.05_full)
# Significant Factors and Items 77-90 and 93-100
data_0.05_reduced <- data_0.05_full  %>% select(contains(c(as.character(c(77:90, 93:100)), 'Sex', 'Breed', 'F_')), Outcome)
cat("Reduced Feature Set with p<0.05\t", dim(data_0.05_reduced))
colnames(data_0.05_reduced)

```

## Multicollinearity 
Calculate Variance Inflation Factor (VIF) to avoid multicollinearity. 
The glm models were built on the Full and Reduced Feature Sets with p < 0.05 and excude any sample containing NAs.
VIF larger than 5 or 10 is large and indicate a high level of multicollinearity.


The resulting multivariate logistic regression models are NOT looking good, p_values are all roughly 1.
Mainly because they have small sample size (examples with NAs were removed).
VIF values indicate a high degree of multicollinearity. 
They were much larger for the full model compared to the reduced model.

```{r, echo = FALSE}
print("VIF full")
model_lr_full <- glm(Outcome ~ ., data=data_0.05_full, family = binomial(link = logit), na.action = na.exclude)
summary(model_lr_full)
# N = 41 samples do NOT contain NAs in the 0.05 FULL dataset, M = 29 features
vif_full <- VIF(model_lr_full)
feat_full_mc <- names(vif_full[vif_full > 5])
feat_full_id <- names(vif_full[vif_full < 5])
print(vif_full)

cat("VIF reduced")
model_lr_reduced <- glm(Outcome ~ ., data=data_0.05_reduced, family = binomial(link = logit), na.action = na.exclude)
summary(model_lr_reduced)
# N = 47 samples do NOT contain NAs in 0.05 REDUCED dataset, M = 12 features
vif_reduced <- VIF(model_lr_reduced)
feat_redu_mc <- names(vif_reduced[vif_reduced > 5])
feat_redu_id <- names(vif_reduced[vif_reduced < 5])
print(vif_reduced)
```

## Visualization
I think for the factors, because they are continuous variables (average of discrete variables), it is better to only have the boxplot.
```{r, echo = FALSE}

plotseries <- function(var){
    plot1 <- ggplot(prq, aes_string( x = var, fill = 'Outcome')) + 
            geom_bar(position = "dodge") +
            theme(legend.position = c(0.13,0.88)) +
            scale_x_discrete("Rating", limits = c(0:4), breaks = c(0:4), labels = c(0:4)) +
            expand_limits(x = c(0,4))

    plot2 <- ggplot(prq, aes_string( x = 'Outcome', y = var)) + 
            geom_boxplot(position = "dodge", aes(colour = Outcome))  +
            theme(legend.position = "none") +
            scale_y_discrete("Rating", limits = c(0:4), breaks = c(0:4), labels = c(0:4)) +
            expand_limits(y = c(0,4)) +
            ylab("Rating")
    plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO")
    

    title <- ggdraw() + 
      draw_label(gsub("[.]", " ", var), fontface = 'bold')
    
    plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))

    suppressMessages(ggsave(paste("output//model-prq//", gsub("[.]", "-", var), ".png", sep = "") 
          , plot))
    
    plot
}

feat_reduced <- colnames(data_0.05_reduced)[1:length(data_0.05_reduced)-1]
lapply(feat_reduced, plotseries)

```

# Multivariate Logistic Regression Model
## Preprocessing (Normalization + KNN Imputation + PCA) and Feature Selection (RFE)
It only uses features selected considering the previous univariate logistic regression pvalue and deviance analysis.
All variables from both the full and reduced features sets had VIF > 5, therefore PCA was performed using the entire feature sets. 
The "recipes" package was used to perform preprocessing (missing value imputation + normalization + PCA) 
[link](https://recipes.tidymodels.org/reference/step_pca.html) 
[Recipes documentation](https://cran.r-project.org/web/packages/recipes/recipes.pdf)
[Example Recipes](https://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)


Features selection was performed using the Caret package Recursive Feature Elimination rfe function.
Logistic Regression was used to model the dataset and Leave One Subject Out Cross Validation was employed to validate the results.
[Caret RFE](https://search.r-project.org/CRAN/refmans/caret/html/rfe.html), 
[Caret RFE Control](https://search.r-project.org/CRAN/refmans/caret/html/rfeControl.html), 
[Examples using RFE with Recipes](http://topepo.github.io/caret/recursive-feature-elimination.html#rferecipes)
[Example RFE + RF as predictor](https://towardsdatascience.com/effective-feature-selection-recursive-feature-elimination-using-r-148ff998e4f7), 
[RFE + ROC as metric](https://stackoverflow.com/questions/18242692/r-package-caret-rfe-function-how-to-customize-metric-to-use-auc).

I did not manage to make the preprocessing work within rfe using LOOCV, it only works with repeatedCV.
Therefore, I had to implement the preprocessing steps (normalization, imputation and pca) using recipes steps and then 
use that transformed dataframe to learn using RFE + LR.
<!-- This results in dataleakage in the ROC estimations using LOOCV.  -->
The resulting models using LOOCV and repeatedCV had PC01 and sometimes PC05 and ROC varying between 0.80 and 0.87, depending on the number of repeats. 

```{r definition, echo = FALSE}

impute_pca <- function(df){

    print("Dimensions of original dataset:")
    print(dim(df))

    model_pca <- recipe(formula =  ~ ., data = df) %>%
                    step_normalize(all_numeric()) %>%
                    step_impute_knn(all_numeric(), neighbors = 3) %>%
                    step_pca(all_numeric(), threshold = .95)

    model_prep <- prep(model_pca, training = df)
    return(model_prep)
}

rfe_lr <- function(df){

    # Setting ROC as the metric for the Logistic Regression function
    lrFuncs$summary <- twoClassSummary
    set.seed(77)    

    ctrl <- rfeControl(functions = lrFuncs,     # logistic regression
                        method = "LOOCV",       # Leave One Out Cross Validation
                        verbose = FALSE)

    # Recursive Feature Elimination with feat_lr
    model <- rfe(Outcome ~ . ,                  # predict Outcome using all other variables
                    data = df,                  # selecting the features from univariate lr
                    sizes = c(1:length(df)),
                    rfeControl = ctrl,
                    metric = "ROC") 
    
    # Printing outputs
    warnings()
    print(model)
    print(summary(model$fit))
    
    return(model)
}


dev_null_residual <- function(model){
    dev = model$null.deviance - model$deviance
    deg = model$df.null - model$df.residual
    cat('\ndeviance difference: ', dev)
    cat('\ndf difference: ', deg)
    cat('\nlevel of significance: ', pchisq(dev, deg, lower.tail = FALSE))
    cat('\nthe model is a good fit: ', pchisq(dev, deg, lower.tail = FALSE) < 0.05)
}

```
## Comparing models
Both Full and Reduced feature sets were used for this analysis. The results obtained were:

- Full -> ROC: 0.81, TPR: 0.92, TNR: 0.36, AIC: 45.038
- Reduced -> ROC: 0.75, TPR: 0.98, TNR: 0.36, AIC: 44.661

Interpreting performance metrics:

- Sensitivity (TPR) = positive results for fail dogs. How many withdrawn dogs were flaged as fail?
- Type 1 error = false positive rate: 1-TPR = 8% of the dogs flagged as fail would successed.

- Specificity (TNR) = negative result for success dogs. How many graduate dogs were flaged as success?
- Type 2 error = false negative error: 1-TNR = 64% of the dogs flagged as success would fail.

The results reveals that the model learnt on the full dataset delivers a better AUC-ROC 0.81 compared to 0.75.
On the other hand, the TPR or Sensitivity is 98% for the Reduced model and 92% for the Full model. 
Therefore, the Reduced model is a better fit for the data. 
The sample size is rather small, therefore that such differences performance may be caused for just a few dogs being correctly classified.
Regardless, a simpler model is always a better choice if the differences in performance are not significant.
Another metric that can be used to compare model is AIC (Akaike Information Criterion), an optimal model minimizes AIC.

Conclusively, the AIC indicates that the Reduced model is a better fit.

```{r, echo = FALSE}
print("p<0.05 + full")
data_0.05_full_prep <- impute_pca(data_0.05_full)
data_0.05_full_pca <- bake(data_0.05_full_prep, data_0.05_full)
rfe_0.05_full <- rfe_lr(data_0.05_full_pca)
dev_null_residual(summary(rfe_0.05_full$fit))
ggplot(data = rfe_0.05_full, metric = "ROC") + theme_bw()

print("p<0.05 + reduced")
data_0.05_reduced_prep <- impute_pca(data_0.05_reduced)
data_0.05_reduced_pca  <- bake(data_0.05_reduced_prep, data_0.05_reduced)
rfe_0.05_reduced <- rfe_lr(data_0.05_reduced_pca)
dev_null_residual(summary(rfe_0.05_reduced$fit))
ggplot(data = rfe_0.05_reduced, metric = "ROC") + theme_bw()

```

## Interpretation: Reduced Feature Set Model

The best Logistic Regression model (Reduced feature set model) is used to investigate the effect of the predictors on the training outcome.
The estimates were exponentiated to get the Odds Ratio (OR). The 95% Confident Intervals (CI) of the estimates were also calculated. 
The OR for PC01 is 1.985, this means that the probability of withdrawal increased by 98.5% for every unit increase of PC1.
<!-- Use Wald test to test the significance of the slope  -->

```{r, echo = FALSE}
# exponentiate the coefficentes to get the odd ratio and their confidence intervals
## odds ratios and 95% CI using profiled log-likelihood
# results_0.05_full <- as.data.frame(coef(summary(rfe_0.05_full$fit)))
# results_0.05_full$OR <- exp(results_0.05_full$Estimate)
# results_0.05_full <- cbind(results_0.05_full, confint(rfe_0.05_full$fit, level = 0.95))
# print("Results from the model create with the full feature set")
# print(results_0.05_full)
```

```{r, echo = FALSE}
results_0.05_reduced <- as.data.frame(coef(summary(rfe_0.05_reduced$fit)))
results_0.05_reduced$OR <- exp(results_0.05_reduced$Estimate)
results_0.05_reduced <- cbind(results_0.05_reduced, exp(confint(rfe_0.05_reduced$fit, level = 0.95)))
print("Results from the model created with the reduced feature set")
print(results_0.05_reduced)
```

```{r, echo = FALSE}
perf <- as.data.frame(data_0.05_reduced_pca %>% select(PC01,Outcome))
perf$Probability <- predict(rfe_0.05_reduced$fit,            # model
                newdata = data_0.05_reduced_pca,        # dataframe, this will only select PC01
                type  = "response")
perf$Predicted <- ifelse(perf$Probability > 0.5, "Fail", "Success")


ggplot(perf, aes(x = PC01, y = Probability, color = Outcome )) +
    # bottom - green
    geom_rect(aes(xmin = -2.5, xmax = 0, ymin = 0, ymax = 0.375), 
            fill = "#99ff99", alpha = 0.2, color = "white") + 
    # top - orange
    geom_rect(aes(xmin = 5, xmax = 7.5, ymin = 0.625, ymax = 1), 
            fill = "#ffff99", alpha = 0.2, color = "white") +  
    geom_point() +
    scale_color_brewer(palette = "Set2")
perf
```

## Feature Importance: Reduced Feature Set Model
Analysis of the principal component loadings that were used in the best Multivariate Logistic Regression Model.

The 5 highest loading values in PC1 come from the variables 
"F_12 Excitability"
"F_03 Dog directed aggression"
"X93 Stares intently at nothing visible"
"F_04 Dog directed fear"
"F_10 Separation related problems"
"X77 Escapes or would escape from home or yard given the chance".
This indicates that this principal component places most variation in these variables. 

```{r, echo = FALSE}
# model prep is the recipe steps, select pca -> number = 3, because it's the 3rd step
df_pca_reduced <- as.data.frame(tidy(data_0.05_reduced_prep, number = 3, type = "coef"))
# creating another dataframe with PC1 values for the analysis
df_pcs_reduced <- df_pca_reduced  %>% filter(component == 'PC1') %>% select(terms, value, component)

# figuring out the righest loading items for the PC1 and PC5 
df_pcs_reduced %>% filter(component == 'PC1') %>% arrange(desc(abs(value))) %>% select(terms, value)

ggplot(df_pcs_reduced, aes(x=unlist(lapply(strsplit(terms,"[.]"), function(x) x[1])), y=value)) +
    geom_bar(stat='identity', position='dodge', width = 0.8) +
    xlab("C-BARQ items or factors") +
    ylab("Principal components") +
    theme(axis.text.x = element_text(angle = 45), legend.position = "top")
print(df_pcs_reduced$terms[1:12])
```

```{r, echo = FALSE}
l <- unlist(data_0.05_reduced[1,1:12]) * df_pcs_reduced$value
sum(l)
data_0.05_reduced_pca 
```


## Conclusion

This model could be used to flag dogs likely to be withdrawn from training for behavioural reasons. 
Dog trainers could consider the output to re-assess and adjust the training programme for flagged dogs in order to address behavioural issues.
Considering the higher TPR achieved by the Reduced model, that might be a better choice when employing this system to correcly identify unsuitable dogs for the training programme.

Duration of dog correctly classified as Fail and went on to Fail in days (Dog 16 = 100, Dog42 = 55, Dog51 = 113, Dog51 = 90), 30, 43, 20 days of training saved! 
70 days for this assessment it could be done earlier too. 
Ideally assign a dog to a dog trainer from start to Week 3. They are them responsible for filling out the questionnaire.
