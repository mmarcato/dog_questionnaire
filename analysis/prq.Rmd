---
title: "Puppy Raiser Questionnaire vs Outcome (Success, Fail)"
author: "Marinara Marcato"
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/marinara.marcato/Project/Scripts/dog_questionnaires")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
#install.packages("")
library(DescTools)
library(ggplot2)
library(cowplot)
library(plyr)
library(dplyr)
library(tidyverse)
library(rstatix)
library(ggpubr)
library(ggsci) # colors for the graphs - npg for nature


# feature selection using recursive feature elimination (RFE)
library(mlbench)
library(caret)
library(reshape2)
library(recipes)
```

# Introduction 
In order to objectively evaluate trainee guide dogs personality, their puppy raisers filled out the standardised questionnaire Canine Behavioural Assessement & Research Questionnaire (C-BARQ) around the time they were due to start formal training.
This document shows the data analysis carried out to investigate the association between ratings given by the puppy raisers and the dog's training outcome.
 Statistical methods will be used to test the hypothesis that there is a relationship between personality ratings and training outcome (Success, Fail).
# Data Exploration

Importing data and converting variables to adequate data types.
```{r, echo = FALSE}
prq = read.csv('data/2022-05-10-PRQ_C-BARQ.csv', stringsAsFactors=TRUE)
# colnames(prq)
# converting date types
prq$Timestamp = as.Date(prq$Timestamp, format= "%Y-%m-%d")
prq$DOB = as.Date(prq$DOB, format= "%Y-%m-%d")
prq$DOA = as.Date(prq$DOA, format= "%Y-%m-%d")
prq$End.Date = as.Date(prq$End.Date, format= "%Y-%m-%d")
prq$Duration = as.numeric(gsub(" .*$", "", prq$Duration))
```

## Demographics

The number of dogs in the Puppy Raiser Questionnaire dataset. Selection criteria: dogs who successed training (assistance and guide dogs) and dogs withdrawn for behavioural reasons.
```{r, echo = FALSE}
cat('Number of dogs:', length(prq$Code))
cat('Training Outcome:')
table(prq$Outcome)
```

Analysing categorical demographic data: Sex, Breed. There was only one German Shepherd and Golden Doodle dogs in the sample, their breeds were relabeled as "Other" for the data analysis.
```{r, echo = FALSE}
print(table(prq$Sex))
print("Original classes")
table(prq$Breed)
# merging breed categories
levels(prq$Breed)[levels(prq$Breed) == "LRx"] <- "LRxGR"
levels(prq$Breed)[levels(prq$Breed) =="GS" | levels(prq$Breed) =="GRxPoodle"] <- "Other"
print("Processed classes")
table(prq$Breed)
```

Analysing age at arrival to the training centre and age at assessment when the questionnaire was completed:

```{r, echo = FALSE}
n <- dim(prq)[1]

prq$Age.at.Arrival <- prq$DOA - prq$DOB
mean <- mean(prq$Age.at.Arrival)
std <- sd(prq$Age.at.Arrival)
margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)

cat('Age at Arrival: Mean', round(mean/30.417, 2), 
            'Standard Deviation', round(std/30.417, 2))
            # 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)


prq$Age.at.Assessment <- prq$Timestamp - prq$DOB
mean <- mean(prq$Age.at.Assessment)
std <- sd(prq$Age.at.Assessment)
margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)

cat('Age at Assessment: Mean', round(mean/30.417, 2), 
            'Standard Deviation', round(std/30.417, 2))
            # 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)
```

Calculate statistics of duration of training for the dogs that were withdrawn from training. 
```{r, echo = FALSE}

# Duration of training before withdrawal in weeks
duration <- prq %>% filter(Outcome == "Fail") %>% select(Duration)/7
print('Duration of Training in weeks')
summary(duration)


h <- ggplot(duration, aes(x=Duration)) +
 geom_histogram(binwidth = 1) +
 xlab("Duration (Weeks)") +
 ylab("Number of Dogs") + theme_bw()
 
h
suppressMessages(ggsave("analysis/prq/duration-histogram.png",h))
```

## Descriptive Statistics
The C-BARQ contains 100 items which were scored by the Puppy Raisers in a scale from 'Never', 'Seldom', 'Sometimes', 'Usually', 'Always' and encoded in a scale from 0 to 4. 
Out of the 100 items, 78 items are used to calculate 14 factors and the remaining 22 items are myscellaneous.
The questionnaire data are kept as numeric, rather than being converted to factors, so the information about the ordering is kept.
The descriptive statistics of the questionnaire data shown below are calculated and saved as a csv.  
Missing data is also analysed. 

```{r, echo = FALSE}
data = prq %>% select(starts_with(c('X', 'F_')), Sex, Breed, Outcome)
# LR will predict the probability of the LAST class 
# R defaults to alphabetical order, therefore, between [Fail Success], it will predict Success.
# releveling the independent variable so we will predict Fail instead of Success
data$Outcome <- relevel(data$Outcome, "Success")
print("Items")
colnames(data[1:100])
print("Factors, Demographic and Outcome")
str(data[101:117])
# FACTOR CALCULATIONS
    # 1.'Stranger-directed aggression' score = (questionnaire items 10 + 11 + 12 + 15 + 16 + 18 + 20 + 21 + 22 + 28)/10.
    # 2.'Owner-directed aggression' score = (items 9 + 13 + 14 + 17 + 19 + 25 + 30 + 31)/8.
    # 3.'Dog-directed aggression' = (items 23 + 24 + 26 + 29)/4
    # 4.'Dog-directed fear' = (items 45 + 46 + 52 + 53)/4.
        # Item 33 approached in 
    # 5.'Dog rivalry'(familiar dog aggression) score = (items 32 + 33 + 34 + 35)/4
    # 6.'Trainability' score = (items 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8)/8—remember to reverse scoring order for items 5, 6 & 7 (see above).
        
    # 7.'Chasing' score = (items 27 + 74 + 75 + 76)/4
    # 8.'Stranger-directed fear' score = (items 36 + 37 + 39 + 40)/4
    # 9.'Nonsocial fear' score = (items 38 + 41 + 42 + 44 + 47 + 48)/6
    # 10.'Separation-related problems' score = (items 54 + 55 + 56 + 57 + 58 + 59 + 60 + 61)/8

    # 11.'Touch sensitivity' score = (items 43 + 49 + 50 + 51)/4
    # 12.'Excitability' score = (items 62 + 63 + 64 + 65 + 66 + 67)/6
    # 13.'Attachment/attention-seeking' score = (items  68 + 69 + 70 + 71 + 72 + 73)/6
    # 14.'Energy' score = (items 91 + 92)/2

    # Factors use 1–76 and 91–92
    # Factors do NOT use item 77-90 and 93-100 
```

```{r, echo = FALSE}
# calculate descriptive statistics
stats <- data.frame(do.call(rbind, lapply(data[1:114], summary)))

# missing values, there is something weird about the NAs calculations using summary
stats <- cbind(stats, colSums(is.na(data[1:114])))
colnames(stats)[8] <- "Missing"

# histogram of missing values in original dataset
h <- ggplot(stats, aes(x=Missing)) +
 geom_histogram(binwidth = 1) +
 xlab("Number of Missing values ") +
 ylab("Number of Variables") + theme_bw()

print('Features sorted by number of missing rows')
print(stats[order(stats$Missing, decreasing = TRUE)[1:15], 'Missing', drop = FALSE])

# 28 NAs -> Item 49 nails clipped by someone in the house
# 12-10 NAs -> Because of no other dog  in household -> Items 33,34,32,35

# save descriptive statistics of the dataset to csv
write.csv(stats, "analysis/prq/descriptive-statistics.csv") 
```

## Univariate Logistic Regression
Logistic Regression one-way analysis of variance test was used to investigate whether there was a statistically significant difference between the behaviours reported by Puppy Raisers and the Training Outcome of the dogs.
C-BARQ Correlation with Training Outcome. 

Breeds (p = 0.44 and p = 0.16) and Sex (p = 0.61) were not significantly associated with training outcome.
```{r, echo = FALSE}
# univariate logistic regression using one predictor and outcome
lr_models <- lapply(data[-length(data)], function(x) summary(glm(formula = Outcome ~ x, 
                            data = data, family = binomial(link = logit), na.action = na.exclude)))
lr_result <- lapply(lr_models, function(x) c(coef(x), x$deviance))

## univariate models with 3 dfs (Breed)
lr_breeds <- data.frame(transpose(lr_result[116]))
colnames(lr_breeds) <- c("estimate_0", "estimate_1", "estimate_2", 
                    "se_0", "se_1", "se_2", "z_value_0", "z_value_1", "z_value_2", 
                    "p_value_0", "p_value_1", "p_value_2", "deviance")
print(lr_models[116])
cat('Difference between Other and LR: \t', lr_breeds$p_value_1, '\t -> not significant')
cat('Difference between Other and LRxGR: \t', lr_breeds$p_value_2, '\t -> not significant')

## univariate models with up to 2 dfs (items are ints)
lr_results <- data.frame(do.call(rbind,lr_result[1:115]))
colnames(lr_results) <- c("estimate_0", "estimate_1", 
                    "se_0", "se_1", "z_value_0", "z_value_1",
                    "p_value_0", "p_value_1", "deviance")

# check if the columns were named appropriately 
# lr_models[1]
# lr_results[1,]

## univariate models with 1 df (because the predictor variable is constant)
print("Columns 9,10,13,17,30,37 are CONSTANT, that's why they p_values don't make sense")
lr_results[c(9,10,13,17,30,37),'p_value_1']

# saving univariate logistic regression results to csv
write.csv(lr_results, "analysis/prq/univariate-logistic-regression.csv")
```

Analysing p-values.
```{r, echo = FALSE}
# print variables sorted by pvalue
# lr_results[order(lr_results$p_value_1, decreasing = FALSE), 'p_value_1' , drop = FALSE]
# pvalue analysis
lr_results$significant_0.05 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.05)
lr_results$significant_0.1 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.1)
lr_results$significant_0.2 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.2)
#  lr_results[order(lr_results$p_value_1),]
print("Number of features considering each level of significance using Logistic Regression p-value test")
print(colSums(lr_results[,10:12], na.rm = TRUE))
# inclusion of X96,98 and F3,6 in feature set p<0.2 compared to with p<0.05 
```

Analysing deviance of full model compared to the null model. 
Reduction in deviance must be significant (chi-squared test) for the model to be considered a good fit.
```{r, echo = FALSE}
# deviance analysis (Null deviance: 58.352  on 62  degrees of freedom = N-1)
lr_results$deviance_diff <- 1 - pchisq((58.352 - lr_results$deviance), 1, lower.tail = FALSE)
lr_results$significant_dev <- (lr_results$deviance_diff > 0.95)
# significant differences in deviances
cat('Number of features whose univariate models resulted in a significant difference in deviance: ', sum(lr_results$significant_dev))
```

Data selection based on the univariate logistic regression p-value and deviance analysis. 
This reduced feature set contains all factors and only items that were NOT included in the calculation of ANY factor with p<0.05 (i.e. Items 1,27 an 75 and some myscellaneous items 77-90, 93-100).

```{r, echo = FALSE}
# selecting features for modelling
print("All features with level of significance < 0.05 using Univariate Logistic Regression p-value test")
# print(lr_results[lr_results$significant_0.05 == TRUE, 'p_value_1', drop = FALSE])

# selecting features with p < 0.05 
data_full <- data %>% 
            select(all_of(lr_results %>% filter(significant_0.05 ==TRUE) %>% rownames()), Outcome)
cat("Full Feature Set with p<0.05\t", dim(data_full))
colnames(data_full)

# items that were significant but not used to calculate factors 1, 27, 75 -> "X1", 27, 75,
# items not used to calculate factors 77-90 and 93-100
data_reduced <- data_full  %>% select(contains(c(as.character(c("X1", 27, 75, 77:90, 93:100)), 'Sex', 'Breed', 'F_')), Outcome)
cat("Reduced Feature Set with p<0.05\t", dim(data_reduced))
colnames(data_reduced)

```

## Missing Data

PRQ, missing values per column and per row. Very few data are missing considering the original items selected.
```{r, echo = FALSE}
# original dataset
# dc_col_nas <- unlist(lapply(dc, function (x) sum(is.na(x))))
# table(dc_col_nas)
# dc_row_nas <- apply(dc, 1, function (x) sum(is.na(x)) ) 
# table(dc_row_nas)
# reduced dataset
col_nas <- unlist(lapply(data_reduced, function (x) sum(is.na(x))))
print(col_nas)
row_nas <- apply(data_reduced, 1, function (x) sum(is.na(x)) ) 
table(row_nas)
```


## Visualization
Plotting the items and factors that will be used for modelling.
```{r, echo = FALSE}

plot_discrete <- function(var){
    
    plot1 <- ggplot(data_reduced, aes_string( x = var, fill = 'Outcome', width = 1)) + 
            geom_bar(position = "dodge") +
            theme(legend.position = c(0.84,0.88)) 
            # scale_x_discrete("Rating", limits = c(0:4), breaks = c(0:4), labels = c(0:4)) +
            # expand_limits(x = c(0,4))

    plot2 <- ggplot(data_reduced, aes_string( x = 'Outcome', y = var)) + 
            geom_violin(position = "dodge", width = 0.8, aes(color = Outcome, fill = Outcome), alpha = 0.2) +
            theme(legend.position = "none") +
            geom_boxplot(position = "dodge", aes(colour = Outcome), fill = "white",  width = 0.3) +
            ylab("Rating")
    plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO")

    title <- ggdraw() + 
            draw_label(gsub("[.]", " ", var), fontface = 'bold')
    
    plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))

    suppressMessages(ggsave(paste("analysis/prq/", 
                    gsub("[.]", "-", var), ".png", sep = ""), plot3))
    
    plot
}

feat_items <- data_reduced %>% select(-Outcome, -contains('F_')) %>% colnames
lapply(feat_items, plot_discrete)

plot_continuous <- function(var){

    # bin = (max(data_reduced[var], na.rm = TRUE) - min(data_reduced[var], na.rm = TRUE)) / 5
    max_x <- max(data_reduced[var], na.rm = TRUE)
    min_x <- min(data_reduced[var], na.rm = TRUE)
    bin <- seq(min_x, max_x, (max_x - min_x)/3)
    cat(min_x, max_x, bin)
    
    plot1 <- ggplot(data_reduced, aes_string(x = var, color = 'Outcome')) + 
            geom_histogram(position = "dodge") +
            theme(legend.position = c(0.84,0.88))

    plot2 <- ggplot(data_reduced, aes_string( x = 'Outcome', y = var)) + 
            geom_violin(position = "dodge", width = 0.8, aes(color = Outcome, fill = Outcome), alpha = 0.2) +
            geom_boxplot(position = "dodge", aes(colour = Outcome), fill = "white",  width = 0.3) +
            theme(legend.position = "none") +
            ylab("Rating")
    plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO")

    title <- ggdraw() + 
            draw_label(gsub("S[.]|[.]", " ", var), fontface = 'bold')
    
    plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))

    suppressMessages(ggsave(paste("analysis/prq/", 
                    gsub("[.]", "-", var), ".png", sep = ""), plot3))
    
    plot
}   

feat_factors <- data_reduced %>% select(contains('F_')) %>% colnames
lapply(feat_factors, plot_discrete)

lapply(feat_factors, function (var) {
    print(var)
    max_x <- max(data_reduced[var], na.rm = TRUE)
    min_x <- min(data_reduced[var], na.rm = TRUE)
    cat(min_x, max_x)
    print(seq(min_x, max_x, (max_x - min_x)/5))
})
```

## Multicollinearity 
Calculate Variance Inflation Factor (VIF) to avoid multicollinearity. 
The glm models were built on the Full and Reduced Feature Sets with p < 0.05 and excude any sample containing NAs.
VIF larger than 5 or 10 is large and indicate a high level of multicollinearity.


The resulting multivariate logistic regression models are NOT looking good, p_values are all roughly 1.
Mainly because they have small sample size (examples with NAs were removed).
VIF values indicate a high degree of multicollinearity. 
They were much larger for the full model compared to the reduced model.

```{r, echo = FALSE}
# print("VIF full")
# model_lr_full <- glm(Outcome ~ ., data=data_full, family = binomial(link = logit), na.action = na.exclude)
# summary(model_lr_full)
# # N = 41 samples do NOT contain NAs in the 0.05 FULL dataset, M = 29 features
# vif_full <- VIF(model_lr_full)
# feat_full_mc <- names(vif_full[vif_full > 5])
# feat_full_id <- names(vif_full[vif_full < 5])
# print(vif_full)

cat("VIF reduced")
model_lr_reduced <- glm(Outcome ~ ., data=data_reduced, family = binomial(link = logit), na.action = na.exclude)
summary(model_lr_reduced)
# N = 47 samples do NOT contain NAs in 0.05 REDUCED dataset, M = 12 features
vif_reduced <- VIF(model_lr_reduced)
feat_redu_mc <- names(vif_reduced[vif_reduced > 5])
feat_redu_id <- names(vif_reduced[vif_reduced < 5])
print(vif_reduced)
```

# Multivariate Model
## Preprocessing: Normalization + Imputation + PCA
Only variables that had a p < 0.05 in univariate logistic regression analysis were used as predictors to estimate Outcome using Logistic Regression.
All variables from both the full and reduced features sets had VIF > 5, therefore PCA was performed using the entire feature sets. 
The "recipes" package was used to perform preprocessing (missing value imputation + normalization + PCA) 
[link](https://recipes.tidymodels.org/reference/step_pca.html),
[Recipes documentation](https://cran.r-project.org/web/packages/recipes/recipes.pdf),
[Example Recipes](https://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)

## Recursive Feature Elimination (RFE) + Multivariate Logistic Regression (LR)
Features selection was performed using the Caret package Recursive Feature Elimination rfe function.
Logistic Regression was used to model the dataset and Leave One Subject Out Cross Validation was employed to validate the results.
[Caret RFE](https://search.r-project.org/CRAN/refmans/caret/html/rfe.html), 
[Caret RFE Control](https://search.r-project.org/CRAN/refmans/caret/html/rfeControl.html), 
[Examples using RFE with Recipes](http://topepo.github.io/caret/recursive-feature-elimination.html#rferecipes)
[Example RFE + RF as predictor](https://towardsdatascience.com/effective-feature-selection-recursive-feature-elimination-using-r-148ff998e4f7), 
[RFE + ROC as metric](https://stackoverflow.com/questions/18242692/r-package-caret-rfe-function-how-to-customize-metric-to-use-auc).
<!-- 
I did not manage to make the preprocessing work within rfe using LOOCV, it only works with repeatedCV.
Therefore, I had to implement the preprocessing steps (normalization, imputation and pca) using recipes steps and then 
use that transformed dataframe to learn using RFE + LR.
This results in dataleakage in the ROC estimations using LOOCV.
The resulting models using LOOCV and repeatedCV had PC01 and sometimes PC05 and ROC varying between 0.80 and 0.87, depending on the number of repeats.  -->

```{r definition, echo = FALSE}

impute_pca <- function(df){

    print("Dimensions of original dataset:")
    print(dim(df))

    model_pca <- recipe(formula =  ~ ., data = df) %>%
                    step_normalize(all_numeric()) %>%
                    step_impute_knn(all_numeric(), neighbors = 3) %>%
                    step_pca(all_numeric(), threshold = .95)

    model_prep <- prep(model_pca, training = df)
    return(model_prep)
}

rfe_lr <- function(df){

    # Setting ROC as the metric for the Logistic Regression function
    lrFuncs$summary <- twoClassSummary
    set.seed(77)    

    ctrl <- rfeControl(functions = lrFuncs,     # logistic regression
                        method = "LOOCV",       # Leave One Out Cross Validation
                        verbose = FALSE)

    # Recursive Feature Elimination with feat_lr
    model <- rfe(Outcome ~ . ,                  # predict Outcome using all other variables
                    data = df,                  # selecting the features from univariate lr
                    sizes = c(1:length(df)),
                    rfeControl = ctrl,
                    metric = "ROC") 
    
    # Printing outputs
    warnings()
    print(model)
    print(summary(model$fit))
    
    return(model)
}


dev_null_residual <- function(model){
    dev = model$null.deviance - model$deviance
    deg = model$df.null - model$df.residual
    cat('\ndeviance difference: ', dev)
    cat('\ndf difference: ', deg)
    cat('\nlevel of significance: ', pchisq(dev, deg, lower.tail = FALSE))
    cat('\nthe model is a good fit: ', pchisq(dev, deg, lower.tail = FALSE) < 0.05)
}

```
<!-- ## Comparing models
Both Full and Reduced feature sets were used for this analysis. The results obtained were:

- Full -> ROC: 0.81, TPR: 0.92, TNR: 0.36, AIC: 45.038
- Reduced -> ROC: 0.75, TPR: 0.98, TNR: 0.36, AIC: 44.661 -->

Interpreting performance metrics:

- Sensitivity (TPR) = positive results for fail dogs. How many withdrawn dogs were flaged as fail?
- Type 1 error = false positive rate: 1-TPR = 8% of the dogs flagged as fail would successed.

- Specificity (TNR) = negative result for success dogs. How many graduate dogs were flaged as success?
- Type 2 error = false negative error: 1-TNR = 64% of the dogs flagged as success would fail.
<!-- 
The results reveal that the model learnt on the reduced dataset delivers a better AUC-ROC 0.81 compared to 0.75.
On the other hand, the TPR or Sensitivity is 98% for the Reduced model and 92% for the Full model. 
Therefore, the Reduced model is a better fit for the data. 
The sample size is rather small, therefore that such differences performance may be caused for just a few dogs being correctly classified.
Regardless, a simpler model is always a better choice if the differences in performance are not significant.
Another metric that can be used to compare model is AIC (Akaike Information Criterion), an optimal model minimizes AIC.

Conclusively, the AIC indicates that the Reduced model is a better fit. -->

```{r, echo = FALSE}
print("p<0.05 + reduced")
data_prep <- impute_pca(data_reduced)
data_pca  <- bake(data_prep, data_reduced)
rfe_reduced <- rfe_lr(data_pca)
dev_null_residual(summary(rfe_reduced$fit))

# plot roc per rfe iteration
plot <- ggplot(data = rfe_reduced, metric = "ROC") + theme_bw()
suppressMessages(ggsave(paste("analysis/prq/rfe-roc.png"), plot))
plot

```

## Interpretation

The best Logistic Regression model is used to investigate the effect of the principal components on the training outcome.
The estimates were exponentiated to get the Odds Ratio (OR). The 95% Confident Interval (CI) of the ORs was also calculated. 
The OR for PC01 is 0.4994, this means that the probability of withdrawal decreases by (0.50-1) 50% for every unit increase of PC1.
We are 95% confident that a unit increase in PC01 results in a 29 to 71% (0.29-1 = 0.71, 0.71-1 = 0.29) less odds of being withdrawn from training.
<!-- Use Wald test to test the significance of the slope  -->

```{r, echo = FALSE}
results_reduced <- as.data.frame(coef(summary(rfe_reduced$fit)))
results_reduced$OR <- exp(results_reduced$Estimate)
results_reduced <- cbind(results_reduced, exp(confint(rfe_reduced$fit, level = 0.95)))
print("Results from the model created with the reduced feature set")
print(results_reduced)
```

```{r, echo = FALSE}
coef <- predictors(rfe_reduced)

perf <- as.data.frame(data_pca %>% select(coef,Outcome))
perf$Outcome <- relevel(perf$Outcome, "Fail")
perf$Probability <- predict(rfe_reduced$fit,       # model
                newdata = data_pca,        # dataframe, this will only select PC01
                type  = "response")
perf$Status <- prq$Status
perf <- perf %>% mutate(Status = Status %>% fct_relevel(c("W", "GD", "AD")))
perf$Predicted <- ifelse(perf$Probability > 0.5, "Fail", "Success")
perf$Estimate <- log( 1 / (1/perf$Probability - 1) )

# logit function, calculate y given the probability(x)
y <- function(x){ return(log(1/( 1/x - 1))) }
th_fail = 0.65
th_success = 0.35

print("Probability predicted by the LR using PRQ data vs true Outcome")
plot <- ggplot(perf, aes(x = Estimate, y = Probability, color = Outcome )) +
    # bottom - green
    geom_rect(aes(xmin = -Inf, xmax = y(th_success), ymin = 0, ymax = th_success), 
            fill = "#99ff99", alpha = 0.2, color = "white") + 
    # top - orange
    geom_rect(aes(xmin = y(th_fail), xmax = Inf, ymin = th_fail, ymax = 1), 
            fill = "#ffff99", alpha = 0.2, color = "white") +  
    geom_point() 
plot

suppressMessages(ggsave(paste("analysis/prq/logit-outcome.png"), plot))


print("Probability predicted by the LR using PRQ data vs true Status")
# plot Logit vs Status
plot <- ggplot(perf, aes(x = Estimate, y = Probability, color = Status )) +
    # bottom - green
    geom_rect(aes(xmin = -Inf, xmax = y(th_success), ymin = 0, ymax =  th_success), 
            fill = "#99ff99", alpha = 0.2, color = "white") + 
    # top - orange
    geom_rect(aes(xmin = y(th_fail), xmax = Inf, ymin =  th_fail, ymax = 1), 
            fill = "#ffff99", alpha = 0.2, color = "white") +  
    geom_point()
plot

suppressMessages(ggsave(paste("analysis/prq/logit-status.png"), plot))

cat("Predicted probability <", th_success, " -> Green flag ")
print("Dogs flagged green are likely to SUCCEED, true outcome count and status proportion:")
table(perf %>% filter(Probability < th_success) %>% pull(Outcome))
prop.table(table(perf %>% filter(Probability < th_success) %>% pull(Status)))
cat("Predicted probability >", th_fail, "-> Yellow flag")
print("Dogs flagged green are likely to FAIL, true outcome count and status proportion:")
table(perf %>% filter(Probability > th_fail) %>% pull(Outcome))
prop.table(table(perf %>% filter(Probability > th_fail) %>% pull(Status)))

```

## Feature Importance

### Explained Variance
```{r, echo = FALSE}
# tidy(recipe steps, number = select pca step, type = coef or variance
dc_pca_var <- as.data.frame(tidy(data_prep, number = 3, type = "variance"))
dc_pca_var <- dc_pca_var %>% spread(key = "terms", value = value) %>% select(-c(id))
dc_pca_var$component <- mapvalues(dc_pca_var$component, from = 1:15, 
    to = c('PC01', 'PC02', 'PC03', 'PC04', 'PC05', 'PC06', 'PC07', 'PC08', 'PC09', 'PC10', 'PC11', 'PC12', 'PC13', 'PC14', 'PC15'))
colnames(dc_pca_var) <- make.names(names(dc_pca_var)) # fixing column names with space

cat("PCA features kept in the final model after RFE: ", coef)
print("Explained Variance per Principal Component (Scree Plot)")
dc_pca_var$Included = factor(c("Yes", "No", "No", "No", "No", "No", "No", "No", "No", "No", "No", "No", "No", "No", "No"))
plot <- ggplot(dc_pca_var, aes(x = component, y = percent.variance, fill = Included)) +
    geom_bar(stat='identity', position = 'dodge', width = 0.8)+
    xlab("Principal components") +
    ylab("Explained Variance (%)") 
plot
suppressMessages(ggsave(paste("analysis/prq/pc-explained-variance.png"), plot))

print("Cumulative Explained Variance per Principal Component")
plot <- ggplot(dc_pca_var, aes(x = component, y = cumulative.percent.variance, fill = Included)) +
    geom_bar(stat='identity', position = 'dodge', width = 0.8)+
    xlab("Principal Components") +
    ylab("Cumulative Variance (%)")
plot
suppressMessages(ggsave(paste("analysis/prq/pc-cumulative-variance.png"), plot))

```

### Coefficient Loadings

Analysis of the principal component loadings that were used in the best Multivariate Logistic Regression Model.

The 5 highest loading values in PC1 come from the variables 
"F_12 Excitability"
"F_03 Dog directed aggression"
"X93 Stares intently at nothing visible"
"F_04 Dog directed fear"
"F_10 Separation related problems"
"X77 Escapes or would escape from home or yard given the chance".
This indicates that this principal component places most variation in these variables. 

```{r, echo = FALSE}
# model prep is the recipe steps, select pca -> number = 3, because it's the 3rd step
df_pca_reduced <- as.data.frame(tidy(data_prep, number = 3, type = "coef"))
# creating another dataframe with PC1 values for the analysis
df_pcs_reduced <- df_pca_reduced  %>% filter(component == 'PC1') %>% select(terms, value, component)

# figuring out the righest loading items for the PC1
df_pcs_reduced %>% filter(component == 'PC1') %>% arrange(desc(abs(value))) %>% select(terms, value)

ggplot(df_pcs_reduced, aes(x=unlist(lapply(strsplit(terms,"[.]"), function(x) x[1])), y=value)) +
    geom_bar(stat='identity', position='dodge', width = 0.8) +
    xlab("C-BARQ items and factors - Predictor Variables") +
    ylab("Principal components") +
    theme(axis.text.x = element_text(angle = 45))
    
```

# Conclusion

Completing this questionnaire before training (Week 0) would allow assistance dog training organisations to understand which dog are more suitable and allow them to make informed decisions when analysing which dogs to keep for training considering the results of this objective assessment. 
The model could be used to flag dogs likely to be withdrawn from training for behavioural reasons. 
Dog trainers could consider the output to re-assess and adjust the training programme for flagged dogs in order to address behavioural issues.
Considering the higher TPR achieved by the Reduced model, that might be a better choice when employing this system to correcly identify unsuitable dogs for the training programme.

Duration of dog correctly classified as Fail and went on to Fail in days (Dog 16 = 100, Dog42 = 55, Dog51 = 113, Dog51 = 90), 30, 43, 20 days of training saved! 
70 days for this assessment it could be done earlier too. 
Ideally assign a dog to a dog trainer from start to Week 3. They are them responsible for filling out the questionnaire.

