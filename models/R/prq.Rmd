---
title: "Puppy Raiser Questionnaire vs Outcome (Success, Fail)"
author: "Marinara Marcato"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/marinara.marcato/Project/Scripts/dog_questionnaires")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
#install.packages("")
library(DescTools)
library(ggplot2)
library(cowplot)
library(plyr)
library(dplyr)
library(tidyverse)
library(rstatix)
library(ggpubr)

# feature selection using recursive feature elimination (RFE)
library(mlbench)
library(caret)
library(reshape2)
library(recipes)
```

## Introduction 
This document tests the hypothesis that there is a difference in behaviours reported in the standardised questionnaire Canine Behavioural Assessement & Research Questionnaire (C-BARQ) considering dog's training outcome.

## Data Exploration

Importing data and converting to adequate datatypes
```{r, echo = FALSE}
prq = read.csv('data//2022-05-10-PRQ_C-BARQ.csv', stringsAsFactors=TRUE)
# colnames(prq)
# converting date types
prq$Timestamp = as.Date(prq$Timestamp, format= "%Y-%m-%d")
prq$DOB = as.Date(prq$DOB, format= "%Y-%m-%d")
prq$DOA = as.Date(prq$DOA, format= "%Y-%m-%d")
prq$End.Date = as.Date(prq$End.Date, format= "%Y-%m-%d")
prq$Duration  = as.Date(prq$End.Date, format= "%Y-%m-%d")
```

### Rows: Dogs

The number of dogs in the Puppy Raiser Questionnaire dataset. Selection criteria: dogs with a defined training outcome.
```{r, echo = FALSE}
cat('Number of dogs:', length(prq$Code))
cat('Training Outcome:')
table(prq$Outcome)
```

Analysing categorical demographic data: Sex, Breed. There was only one German Shepherd and Golden Doodle in the sample, their breeds were relabeled as "Other" for the data analysis.
```{r, echo = FALSE}
print(table(prq$Sex))
print("Original classes")
table(prq$Breed)
# merging breed categories
levels(prq$Breed)[levels(prq$Breed) == "LRx"] <- "LRxGR"
levels(prq$Breed)[levels(prq$Breed) =="GS" | levels(prq$Breed) =="GRxPoodle"] <- "Other"
print("Processed classes")
table(prq$Breed)
```

Calculate and analyse age at assessment:
```{r, echo = FALSE}
prq$Age.at.Assessment <- prq$Timestamp - prq$DOB
cat('Age at Assessment: Mean', round(mean(prq$Age.at.Assessment)/30.417, 2), 
            'Standard Deviation', round(sd(prq$Age.at.Assessment)/30.417, 2))
```

### Columns: Features
Selecting features for predictive analysis.
Below are the questionnaire items (variables) that were scored by the Puppy Raisers in a scale from 'Never', 'Seldom', 'Sometimes', 'Usually', 'Always'. 

```{r, echo = FALSE}
data = prq %>% select(starts_with(c('X', 'F_')), Sex, Breed, Outcome)
# LR will predict the probability of the LAST class 
# R defaults to alphabetical order, therefore, between [Fail Success], it will predict Success.
# releveling the independent variable so we will predict Fail instead of Success
data$Outcome <- relevel(data$Outcome, "Success")
print("Items")
colnames(data[1:100])
print("Factors, Demographic and outcome")
str(data[101:117])
# FACTOR CALCULATIONS
    # 1.'Stranger-directed aggression' score = (questionnaire items 10 + 11 + 12 + 15 + 16 + 18 + 20 + 21 + 22 + 28)/10.
    # 2.'Owner-directed aggression' score = (items 9 + 13 + 14 + 17 + 19 + 25 + 30 + 31)/8.
    # 3.'Dog-directed aggression' = (items 23 + 24 + 26 + 29)/4
    # 4.'Dog-directed fear' = (items 45 + 46 + 52 + 53)/4.
        # Item 33 approached in 
    # 5.'Dog rivalry'(familiar dog aggression) score = (items 32 + 33 + 34 + 35)/4
    # 6.'Trainability' score = (items 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8)/8—remember to reverse scoring order for items 5, 6 & 7 (see above).
        
    # 7.'Chasing' score = (items 27 + 74 + 75 + 76)/4
    # 8.'Stranger-directed fear' score = (items 36 + 37 + 39 + 40)/4
    # 9.'Nonsocial fear' score = (items 38 + 41 + 42 + 44 + 47 + 48)/6
    # 10.'Separation-related problems' score = (items 54 + 55 + 56 + 57 + 58 + 59 + 60 + 61)/8

    # 11.'Touch sensitivity' score = (items 43 + 49 + 50 + 51)/4
    # 12.'Excitability' score = (items 62 + 63 + 64 + 65 + 66 + 67)/6
    # 13.'Attachment/attention-seeking' score = (items  68 + 69 + 70 + 71 + 72 + 73)/6
    # 14.'Energy' score = (items 91 + 92)/2

    # Factors use 1–76 and 91–92
    # Factors do NOT use item 77-90 and 93-100 
```

## Descriptive Statistics
Calculate and save the descritive statistics of the features selected for hypothesis testing the association with training outcome. 
Analysis missing data. 
```{r, echo = FALSE}
# calculate descriptive statistics
stats <- data.frame(do.call(rbind, lapply(data[1:114], summary)))

# missing values, there is something weird about the NAs calculations using summary
stats <- cbind(stats, colSums(is.na(data[1:114])))
colnames(stats)[8] <- "Missing"
hist(stats$Missing, breaks = 20)
print('Features sorted by number of missing rows')
print(stats[order(stats$Missing, decreasing = TRUE)[1:15], 'Missing', drop = FALSE])

# 28 NAs -> Item 49 nails clipped by someone in the house
# 12-10 NAs -> Because of no other dog  in household -> Items 33,34,32,35

# save descriptive statistics of the dataset to csv
write.csv(stats, "models/R/prq-stats.csv") 
```

## Univariate Logistic Regression
Logistic Regression one-way analysis of variance test was used to investigate whether there was a statistically significant difference between the behaviours reported by Puppy Raisers and the Training Outcome of the dogs.
C-BARQ Correlation with Training Outcome. 

Breeds and Sex (p = 0.61361)were not significantly associated with training outcome.
```{r, echo = FALSE}
# univariate logistic regression using one predictor and outcome
lr_models <- lapply(data[-length(data)], function(x) summary(glm(formula = Outcome ~ x, 
                            data = data, family = binomial(link = logit), na.action = na.exclude)))
lr_result <- lapply(lr_models, function(x) c(coef(x), x$deviance))
lr_models[115]
## univariate models with 3 dfs (Breed)
lr_breeds <- data.frame(transpose(lr_result[116]))
colnames(lr_breeds) <- c("estimate_0", "estimate_1", "estimate_2", 
                    "se_0", "se_1", "se_2", 
                    "z_value_0", "z_value_1", "z_value_2", 
                    "p_value_0", "p_value_1", "p_value_2", "deviance")
print(unique(data$Breed))
cat('Difference between Other and LR: \t', lr_breeds$p_value_1, '\t -> not significant')
cat('Difference between Other and LRxGR: \t', lr_breeds$p_value_2, '\t -> not significant')

## univariate models with up to 2 dfs (items are ints)
lr_results <- data.frame(do.call(rbind,lr_result[1:115]))
colnames(lr_results) <- c("estimate_0", "estimate_1", "se_0", "se_1", "z_value_0", "z_value_1",
                    "p_value_0", "p_value_1", "deviance")

# check if the columns were named appropriately 
# lr_models[1]
# lr_results[1,]

## univariate models with 1 df (because the predictor variable is constant)
print("Columns 9,10,13,17,30,37 are CONSTANT, that's why they p_values don't make sense")
lr_results[c(9,10,13,17,30,37),'p_value_1']
```

Analysing p-values.
```{r, echo = FALSE}
# print variables sorted by pvalue
# lr_results[order(lr_results$p_value_1, decreasing = FALSE), 'p_value_1' , drop = FALSE]
# pvalue analysis
lr_results$significant_0.05 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.05)
lr_results$significant_0.1 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.1)
lr_results$significant_0.2 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.2)
# lr_results[order(lr_results$p_value_1),]
print("Number of features considering each level of significance using Logistic Regression p-value test")
print(colSums(lr_results[,10:12], na.rm = TRUE))
```

Analysing deviance.
```{r, echo = FALSE}
# deviance analysis (Null deviance: 58.352  on 62  degrees of freedom = N-1)
lr_results$deviance_diff <- 1 - pchisq((58.352 - lr_results$deviance), 1, lower.tail = FALSE)
lr_results$significant_dev <- (lr_results$deviance_diff > 0.95)
# significant differences in deviances
cat('Number of features whose univariate models resulted in a significant difference in deviance: ', sum(lr_results$significant_dev))
```

Data selection based on the univariate logistic regression p-value and deviance analysis. 
Full feature set constains all factors and all items significant p<0.05.
Reduced feature set contains all factors and items NOT included in the calculation of ANY items p<0.05 (i.e. Items 77-90, 93-100).
THINK ABOUT THIS: remove only items whose factors were included in the model?!
```{r, echo = FALSE}
# all of the features with p < 0.2 had significant reduction in deviance too, the filtering by two conditions is NOT necessary in that case
# lr_results %>% filter(p_value_1 <0.2 & significant_dev == TRUE) %>%  select(p_value_1, significant_dev)
# selecting features with p < 0.2
data_0.2_full <- data %>% select(all_of(rownames(lr_results[lr_results$p_value_1 < 0.2 ,])), Outcome)
data_0.2_reduced <- data_0.2_full %>% select(contains(c(as.character(c(77:90, 93:100)), 'Sex', 'Breed', 'F_')), Outcome)
# inclusion of X96,98 and F3,6 compared to feature set with 0.05 
# colnames(data_0.2_reduced)
# dim(data_0.2_reduced)

# print("All features with level of significance < 0.05 using Univariate Logistic Regression p-value test")
# print(lr_results[lr_results$significant_0.05 == TRUE, 'p_value_1', drop = FALSE])

# selecting features with p < 0.05 
data_0.05_full <- data %>% select(all_of(rownames(lr_results[lr_results$significant_0.05 ,])), Outcome)
cat("Full Feature Set with p<0.05\t", dim(data_0.05_full))
colnames(data_0.05_full)
# Significant Factors and Items 77-90 and 93-100
data_0.05_reduced <- data_0.05_full  %>% select(contains(c(as.character(c(77:90, 93:100)), 'Sex', 'Breed', 'F_')), Outcome)
cat("Reduced Feature Set with p<0.05\t", dim(data_0.05_reduced))
colnames(data_0.05_reduced)

```

## Multivariate Logistic Regression Model: Multicollinearity
Calculate Variance Inflation Factor (VIF) to avoid multicollinearity. 
The glm models were built on the Full and Reduced Feature Sets with p < 0.05 and excude any sample containing NAs.
VIF larger than 5 or 10 is large and indicate a high level of multicollinearity.


The resulting multivariate logistic regression models are NOT looking good, p_values are all roughly 1.
VIF values indicate a high degree of multicollinearity. 
They were much larger for the full model compared to the reduced model.

```{r, echo = FALSE}
model_lr_full <- glm(Outcome ~ ., data=data_0.05_full, family = binomial(link = logit), na.action = na.exclude)
# summary(model_lr_full)
# N = 41 samples do NOT contain NAs in the 0.05 FULL dataset, M = 29 features
vif_full <- VIF(model_lr_full)
feat_full_mc <- names(vif_full[vif_full > 5])
feat_full_id <- names(vif_full[vif_full < 5])
print("VIF full")
print(vif_full)

model_lr_reduced <- glm(Outcome ~ ., data=data_0.05_reduced, family = binomial(link = logit), na.action = na.exclude)
# summary(model_lr_reduced)
# N = 47 samples do NOT contain NAs in 0.05 REDUCED dataset, M = 12 features
vif_reduced <- VIF(model_lr_reduced)
feat_redu_mc <- names(vif_reduced[vif_reduced > 5])
feat_redu_id <- names(vif_reduced[vif_reduced < 5])

cat("VIF reduced")
print(vif_reduced)
```


## Multivariate Logistic Regression Model: Preprocessing with KNN Imputation and PCA + Feature Selection with RFE
Only features selected univariate logistic regression model pvalue, deviance analysis.
All variables from both the full and reduced features sets had VIF > 5, therefore PCA was performed using the entire feature sets. 
The "recipes" package was used to perform preprocessing (missing value imputation + normalization + PCA) 
[link](https://recipes.tidymodels.org/reference/step_pca.html) 
[Recipes documentation](https://cran.r-project.org/web/packages/recipes/recipes.pdf)
[Example Recipes](https://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)


Features selection was performed using the Caret package Recursive Feature Elimination rfe function.
Logistic Regression was used to model the dataset and Leave One Subject Out Cross Validation was employed to validate the results.
[Caret RFE](https://search.r-project.org/CRAN/refmans/caret/html/rfe.html), 
[Caret RFE Control](https://search.r-project.org/CRAN/refmans/caret/html/rfeControl.html), 
[Examples using RFE with Recipes](http://topepo.github.io/caret/recursive-feature-elimination.html#rferecipes)
[Example RFE + RF as predictor](https://towardsdatascience.com/effective-feature-selection-recursive-feature-elimination-using-r-148ff998e4f7), 
[RFE + ROC as metric](https://stackoverflow.com/questions/18242692/r-package-caret-rfe-function-how-to-customize-metric-to-use-auc).

I did not manage to make the preprocessing work within rfe using LOOCV, it only works with repeatedCV.
Therefore, there is some dataleakage in the ROC estimations. 
However, the resulting models also had PC01 and sometimes PC05, depending on the number of repeats with ROC varying between 0.80 and 0.87. 

```{r definition, echo = FALSE}

impute_pca <- function(df){

    print("Dimensions of original dataset:")
    print(dim(df))

    model_pca <- recipe(formula =  ~ ., data = df) %>%
                    step_normalize(all_numeric()) %>%
                    # step_impute_mean(all_numeric()) %>%
                    step_impute_knn(all_numeric(), neighbors = 3) %>%
                    step_pca(all_numeric(), threshold = .95)

    model_prep <- prep(model_pca, training = df)
    return(model_prep)
}

rfe_lr <- function(df){

    # Setting ROC as the metric for the Logistic Regression function
    lrFuncs$summary <- twoClassSummary
    set.seed(77)    

    ctrl <- rfeControl(functions = lrFuncs,     # logistic regression
                        method = "LOOCV",       # Leave One Out Cross Validation
                        verbose = FALSE)

    # Recursive Feature Elimination with feat_lr
    model <- rfe(Outcome ~ . ,                  # predict Outcome using all other variables
                    data = df,                  # selecting the features from univariate lr
                    sizes = c(1:length(df)),
                    rfeControl = ctrl,
                    metric = "ROC") 
    
    # Printing outputs
    warnings()
    print(model)
    print(summary(model$fit))
    
    return(model)
}


dev_null_residual <- function(model){
    dev = model$null.deviance - model$deviance
    deg = model$df.null - model$df.residual
    cat('\ndeviance difference: ', dev)
    cat('\ndf difference: ', deg)
    cat('\nlevel of significance: ', pchisq(dev, deg, lower.tail = FALSE))
    cat('\nthe model is a good fit: ', pchisq(dev, deg, lower.tail = FALSE) < 0.05)
}

```

Both Full and Reduced datasets were used for this analysis.

- Full -> ROC: 0.81, TPR: 0.92, TNR: 0.36, AIC: 45.038
- Reduced -> ROC: 0.75, TPR: 0.98, TNR: 0.36, AIC: 44.661

Interpreting performance metrics:

- Sensitivity - positive results for fail dogs. How many withdrawn dogs were flaged as fail?
- Type 1 error = false positive rate: 1-TPR = 8% of the dogs flagged as fail would successed.

- Specificity - negative result for success dogs. How many graduate dogs were flaged as success?
- Type 2 error = false negative error: 1-TNR = 64% of the dogs flagged as success would fail.

The results reveals that the model learnt on the full dataset delivers a better AUC-ROC 0.81 compared to 0.7483.
On the other hand, the TPR or Sensitivity is 98% for the Reduced model and 92% for the Reduced model. 
Therefore, the Reduced model might be a better choice. 
However, it is important to highlight that the sample size is rather smal. 
Such difference in performance may be caused for just a few dogs being correctly classified.


```{r, echo = FALSE}
print("p<0.05 + full")
data_0.05_full_prep <- impute_pca(data_0.05_full)
data_0.05_full_pca <- bake(data_0.05_full_prep, data_0.05_full)
rfe_0.05_full <- rfe_lr(data_0.05_full_pca)
dev_null_residual(summary(rfe_0.05_full$fit))
ggplot(data = rfe_0.05_full, metric = "ROC") + theme_bw()

print("p<0.05 + reduced")
data_0.05_reduced_prep <- impute_pca(data_0.05_reduced)
data_0.05_reduced_pca  <- bake(data_0.05_reduced_prep, data_0.05_reduced)
rfe_0.05_reduced <- rfe_lr(data_0.05_reduced_pca)
dev_null_residual(summary(rfe_0.05_reduced$fit))
ggplot(data = rfe_0.05_reduced, metric = "ROC") + theme_bw()

```

## Multivariate Logistic Regression Model: Output Analysis

The best Logistic Regression model is used to investigate the effect of each variable on the training outcome.
After exponentiating the Odds Ratio and 95% Confident Intervals (CI). Add interpretation here.

```{r, echo = FALSE}
# exponentiate the coefficentes to get the odd ratio and their confidence intervals
## odds ratios and 95% CI using profiled log-likelihood
print(round(exp(cbind(OR = coef(rfe_0.05_full $fit), confint(rfe_0.05_full $fit, level = 0.95))),4))
print(round(exp(cbind(OR = coef(rfe_0.05_reduced $fit), confint(rfe_0.05_reduced $fit, level = 0.95))),4))
```

## Feature Importance
Analysis of the principal component loadings that were used in the best Multivariate Logistic Regression Model.

### Full Feature Set
The 5 highest values in PC1 are 
"X26 Toward unfamiliar dogs visiting your home", 
"X45 When approached directly by an unfamiliar dog of the same or larger size",
"X29 When barked growled or lunged at by another unfamiliar dog", 
"F_03 Dog directed aggression", 
"F_04 Dog directed fear"
which indicate that this principal component places most variation in these variables related to fearful and agressive behaviour.

The 5 highest values in PC5 are
"X75 Chases or would chase birds given the opportunity"
"X77 Escapes or would escape from home or yard given the chance"
"X99 Licks people or objects excessively"
"X82 Begs persistently for food when people are eating"
"F_09 Nonsocial fear"
which indicate that this principal component places most variation in these variables. 
(chasing, escaping, licking, food drive, nonsocial fear)

```{r, echo = FALSE}
# model prep is th receipe steps, for the pca, number = 3, because it's the 3rd step
df_pca_full <- as.data.frame(tidy(data_0.05_full_prep, number = 3))
# creating another dataframe with PC1 and PC5 values for the analysis
df_pcs_full <- df_pca_full %>% filter(component == 'PC1' | component == 'PC5') %>% select(terms, value, component)

# figuring out the righest loading items for the PC1 and PC5 
print("PC1 and PC2 values")
df_pcs_full %>% filter(component == 'PC1') %>% arrange(desc(abs(value))) %>% select(terms, value)
df_pcs_full %>% filter(component == 'PC5') %>% arrange(desc(abs(value))) %>% select(terms, value)

ggplot(df_pcs_full, aes(x=unlist(lapply(strsplit(terms,"[.]"), function(x) x[1])), y=value, fill=component)) +
    geom_bar(stat='identity', position='dodge', width = 0.8) +
    xlab("C-BARQ items or factors") +
    ylab("Principal components") +
    theme(axis.text.x = element_text(angle = 45), legend.position = "top")
print(df_pcs_full$terms[1:29])
```

### Reduced Feature Set
The 5 highest values in PC1 are 
"F_12.Excitability"
"F_03.Dog.directed.aggression"
"X93.Stares.intently.at.nothing.visible"
"F_04.Dog.directed.fear"
"F_10.Separation.related.problems"
"X77.Escapes.or.would.escape.from.home.or.yard.given.the.chance"
which indicate that this principal component places most variation in these variables. 

```{r, echo = FALSE}
# model prep is th receipe steps, for the pca, number = 3, because it's the 3rd step
df_pca_reduced <- as.data.frame(tidy(data_0.05_reduced_prep, number = 3))
# creating another dataframe with PC1 and PC5 values for the analysis
df_pcs_reduced  <- df_pca_reduced  %>% filter(component == 'PC1') %>% select(terms, value, component)

# figuring out the righest loading items for the PC1 and PC5 
df_pcs_reduced %>% filter(component == 'PC1') %>% arrange(desc(abs(value))) %>% select(terms, value)

ggplot(df_pcs_reduced, aes(x=unlist(lapply(strsplit(terms,"[.]"), function(x) x[1])), y=value)) +
    geom_bar(stat='identity', position='dodge', width = 0.8) +
    xlab("C-BARQ items or factors") +
    ylab("Principal components") +
    theme(axis.text.x = element_text(angle = 45), legend.position = "top")
print(df_pcs_reduced$terms[1:12])
```

## Conclusion

This model could be used to flag dogs likely to be withdrawn from training for behavioural reasons. 
Dog trainers could consider the output to re-assess and adjust the training programme for flagged dogs in order to address behavioural issues.
Considering the higher TPR achieved by the Reduced model, that might be a better choice when employing this system to correcly identify unsuitable dogs for the training programme.