---
title: "Dog Trainer Questionnaire vs Outcome (Fail, Success) "
author: "Marinara Marcato"
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/marinara.marcato/Project/Scripts/dog_questionnaires")
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE) 
#install.packages("ggpubr")

library(DescTools)
# plot
library(ggplot2)
library(ggpubr)
library(cowplot)
# datasets
library(plyr)
library(dplyr)
library(tidyverse)
library(reshape2) # remove?

library(ggsci)  # colors for the graphs - png colours for nature
library(broom)  # convert r output into tibbles
library(xtable) # latex table output

# machine learning/stats
library(mlbench) # remove?
library(recipes)
library(caret)
library(PRROC)  # calculates precision recall curve
```

```{r output, include = FALSE, results = "hide", message = FALSE}
# save dataframes to csv and print LaTeX table
tab_results <- function(df, name, caption=NULL) {

    # remove the dots from the rownames
    rownames(df) <- gsub("[.]", " ", rownames(df))
    
    path = paste("results/dtq/", name, ".csv", sep = "")

    # save csv file
    write.csv(df, path)
    cat("dataframe saved to ", path)
    
    # latex code for table
    # print(xtable(df,
    #     caption = paste(caption, ".", sep = ""),
    #     label = paste("T-dtq-", name, sep = "")),
    #     caption.placement = "top")
}

# save images as png and print LaTeX table
fig_results <- function(plot, name, caption = NULL, label = NULL){

    path = paste("results/dtq/", name, ".png", sep = "")
    
    # if label is not given, use name
    if (is.null(label)){
        label = name
    }
    # save png
    suppressMessages(ggsave(filename = path, 
                plot = plot + theme(text = element_text(size = 20)),
                width = 8, height = 8))
    cat("Image saved at", path)

    # latex code for image
    # cat("\n\nStart LaTeX code\n\n",
    #         paste("\\begin{figure}[!h]\n\\centering\n\\caption{", paste(caption, ".", sep = ""),"}","\n\\label{F-dtq-", label, "}\n\\includegraphics[width = 11cm]{",path,"}\n\\end{figure}", sep = ""), 
    #         "\n\nEnd LaTeX Code\n\n")
}
```


# Introduction 
In order to objectively evaluate trainee guide dogs personality, their dog trainers filled out the standardised questionnaire Monash Canine Personality Questionnaire - Revised (MCPQ-R) after 10 weeks of training. 
This document shows the data analysis carried out to investigate the association between the ratings given by trainers and the dogâ€™s training outcome. 
Statistical methods will be used to test the hypothesis that there is a relationship between personality ratings and training outcome (Success, Fail).

# Data Exploration

Importing data and converting variables to adequate data types.
```{r, echo = FALSE}
dtq = read.csv('data//2022-06-27-DTQ_MCPQ-R.csv', stringsAsFactors=TRUE)
colnames(dtq)
# converting date types
dtq$Timestamp = as.Date(dtq$Timestamp, format= "%Y-%m-%d")
dtq$DOB = as.Date(dtq$DOB, format= "%Y-%m-%d")
dtq$DOA = as.Date(dtq$DOA, format= "%Y-%m-%d")
dtq$End.Date = as.Date(dtq$End.Date, format= "%Y-%m-%d")
dtq$Duration = as.numeric(gsub(" .*$", "", dtq$Duration))
```
## Demographics
The number of dogs in the Dog Trainer Questionnaire dataset. Selection criteria: dogs who successed training (assistance and guide dogs) and dogs withdrawn for behavioural reasons.
```{r, echo = FALSE}
cat('Number of dogs:', length(dtq$Code))
cat('Training Outcome:')
table(dtq$Outcome)
```

Analysing categorical demographic data: Sex, Breed. There was only one German Shepherd and two Golden Doodle dogs in the sample, their breeds were relabeled as "Other" for the data analysis.
```{r, echo = FALSE}
print(table(dtq$Sex))
print("Original classes")
table(dtq$Breed)
# merging breed categories
levels(dtq$Breed)[levels(dtq$Breed) == "LRx"] <- "LRxGR"
levels(dtq$Breed)[levels(dtq$Breed) =="GS" | levels(dtq$Breed) =="GRxPoodle"] <- "Other"
print("Processed classes")
table(dtq$Breed)
```

Analysing age at arrival to the training centre and age at assessment when the questionnaire was completed:
```{r, echo = FALSE}
n <- dim(dtq)[1]

dtq$Age.at.Arrival <- dtq$DOA - dtq$DOB
mean <- mean(dtq$Age.at.Arrival)
std <- sd(dtq$Age.at.Arrival)
margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)

cat('Age at Arrival: Mean', round(mean/30.417, 2), 
            'Standard Deviation', round(std/30.417, 2))
            # 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)


dtq$Age.at.Assessment <- dtq$Timestamp - dtq$DOB
cat('Age at Assessment: Mean', round(mean(dtq$Age.at.Assessment)/30.417,2), 
            'Standard Deviation', round(sd(dtq$Age.at.Assessment)/30.417, 2))


dtq$Duration.at.Assessment <- dtq$Timestamp - dtq$DOA
cat('Duration at Assessment: Mean', round(mean(dtq$Duration.at.Assessment)/30.417,2), 
            'Standard Deviation', round(sd(dtq$Duration.at.Assessment)/30.417, 2))
summary(as.integer(dtq$Duration.at.Assessment)/7)
hist(as.integer(dtq$Duration.at.Assessment)/7)

```

Calculate statistics of duration of training for the dogs that were withdrawn from training. 
```{r, echo = FALSE}
# Duration of training before withdrawal in weeks
# dtq %>% arrange(Duration) %>% select(Duration, Timestamp, DOA, Name)
duration <- dtq %>% filter(Outcome == "Fail") %>% select(Duration)/7
print('Duration of Training in weeks')
summary(duration)

h <- ggplot(duration, aes(x=Duration)) +
 geom_histogram(binwidth = 1) +
 xlab("Duration (Weeks)") +
 ylab("Number of Dogs") + theme_bw()
 
h

# dtq %>% filter(Outcome == "Fail") %>% nrow() # number of dogs that failed
fig_results(h, name = "duration-histogram",
            caption = "Duration of training in weeks for dogs that were withdrawn from training for behavioural reasons.")
```

## Descriptive Statistics
The MCPQ-R contains 26 items which were scored by the Trainer in a scale from 1 to 6.
The questionnaire data are kept as integers, rather than being converted to factors, so the information about the ordering is kept.
The descriptive statistics of the questionnaire data shown below are saved as a csv.  

<!-- 
Other potential variables for inclusion in the model:
"PR.Sup" -> Alison = 5, Catherine = 6, DSM = 3, Frances = 11, Graham = 24, Mags = 3, Rose = 23, UKPR = 11
"Source" -> IGDB = 60, AADI =3, Cesecah = 1, GDBUK = 16, Private breeder = 8, PRV breeder = 1 -->

```{r, echo = FALSE}
# selecting features for modelling
data = dtq %>% select(contains(c("Extraversion", "Motivation", "Training",
"Amicability", "Neuroticism", "Sex", "Breed", "Outcome")))  %>% select(!contains("Comments"))
colnames(data)
cat("Dimension of the dataset including the dependent variable: ", dim(data))
# str(data)
# calculate descriptive statistics on for the numeric factors
stats <- data.frame(do.call(rbind, lapply(data %>% select(where(is.numeric)), summary)))
print(stats)
# lapply(data %>% select(where(is.factor)), summary)
# save descriptive statistics of the dataset to csv
tab_results(stats, name = "descriptive-statistics", 
                caption = "\acrshort{mcpqr} rating descriptive statistics including minimum, 1st quartile, median, mean, 3rd quartile, maximum and number of missing values for all items and factors.") 
```

## Feature Selection
### Criteria 1 and 2: Univariate Logistic Regression
Logistic regression was used to investigate whether there was a statistically significant difference between the behaviours reported by Dot Trainers and the Training Outcome of the dogs.
MCPQ-R Correlation with Training Outcome. 

Breeds (p = 0.82 and p = 0.40) and Sex (p = 0.32) were not significantly associated with training outcome.
```{r, echo = FALSE}
# GLM function in R will predict the probability of the LAST class 
# R defaults to alphabetical order, therefore, between [Fail Success], it will predict Success
# releveling to have Success as FIRST class so Fail is the last class which will be predicted
data$Outcome <- relevel(data$Outcome, "Success")
# univariate logistic regression using one predictor and outcome
lr_models <- lapply(data %>% select(-Outcome), 
                        function(x) glm(formula = Outcome ~ x, 
                        data = data, 
                        family = binomial(link = logit)))
lr_result <- lapply(lr_models, function(x) c(coef(summary((x)), summary(x)$deviance),4))

## univariate models with 3 dfs (Breed)
print(lr_models[28])
lr_breeds <- data.frame(transpose(lr_result[28]))
colnames(lr_breeds) <- c("estimate_0", "estimate_1", "estimate_2", 
                    "se_0", "se_1", "se_2", "z_value_0", "z_value_1", "z_value_2", 
                    "p_value_0", "p_value_1", "p_value_2", "deviance")
# print(unique(data$Breed))
cat('Difference between Other and LR: \t', lr_breeds$p_value_1, '\t -> not significant')
cat('Difference between Other and LRxGR: \t', lr_breeds$p_value_2, '\t -> not significant')

## univariate models with up to 2 dfs
lr_results <- data.frame(do.call(rbind,lr_result[1:27]))
colnames(lr_results) <- c("estimate_0", "estimate_1", 
                    "se_0", "se_1", "z_value_0", "z_value_1",
                    "p_value_0", "p_value_1", "deviance")
lr_results %>% filter(p_value_1 < 0.05) %>% select(estimate_1, p_value_1)
cat('Difference between Sex: \t', lr_results$p_value_1[27], '\t -> not significant')

# saving univariate logistic regression results to csv
tab_results(lr_results, name = "univariate-logistic-regression")
```

Converting logistic regression result to odds ratio and confidence intervals. 
```{r, echo = FALSE}
lr <- lr_results %>% select(estimate_1, p_value_1) # se_1, z_value_1,
colnames(lr) <- c("Estimate", "p-value")
lr$OR <- exp(lr$Estimate)
# list of dataframes of CI for each of the models on the list-length(lr_models)
lr_cis <- lapply(lr_models[-length(lr_models)], function(x) as.data.frame(exp(confint(x, level = 0.95))))
lapply(lr_models[1], function(x) (confint(x, level = 0.95)))
confint(summary(lr_models$Extraversion.Active))
# list of dataframes removing the CI for the intercept
lr_cis <- lapply(lr_cis, function(x) x[2,])
# turn list of dataframes into a dataframe
lr_ci <- bind_rows(lr_cis, .id = 'x')
# replace rownames with column with the name of the variables
lr_ci <- lr_ci %>% remove_rownames %>% column_to_rownames(var="x")

# bind CIs
lr <- cbind(lr, lr_ci)

# save csv and print latex code
tab_results(lr, name = "lr-or-ci",
    caption = "Univariate logistic regression model estimates, p-value, odds ratio (OR) and confidence interval (CI)")
```

Analysing p-values and deviance.
Analysing deviance of full model compared to the null model. Reduction in deviance must be significant (chi-squared test) for the model to be considered a good fit.
```{r, echo = FALSE}
# CRITERION 1: pvalue analysis
lr_results$pvalue_sign <- (lr_results$p_value_1 < 0.05) 
# significant differences in deviances
cat('Number of features whose univariate models resulted in a significant difference in pvalue: ', sum(lr_results$pvalue_sign))

# CRITERION 2: deviance analysis (Null deviance: 89.623  on 88  degrees of freedom = N-1)
lr_results$deviance_diff <- pchisq((89.623 - lr_results$deviance), 1, lower.tail = FALSE)
lr_results$deviance_sign <- (lr_results$deviance_diff < 0.05)
# significant differences in deviances
cat('Number of features whose univariate models resulted in a significant difference in deviance: ', sum(lr_results$deviance_sign))

```

Feature selection based on the univariate logistic regression p-value and deviance analysis. 
This reduced feature set contains all factors and only items that were NOT included in the calculation of statistically significant factor with p<0.05 (i.e. Items 1,27 an 75 and some myscellaneous items 77-90, 93-100).
```{r, echo = FALSE}
# CRITERIA 1 and 2
print("All features with level of significance < 0.05 using Univariate Logistic Regression p-value test")
data_2 <- data %>% 
    select(all_of(lr_results %>% filter(pvalue_sign ==TRUE & deviance_sign == TRUE) %>% rownames()), Outcome)
cat("Full Feature Set with p<0.05 and reduction in deviance\t", dim(data_2))
colnames(data_2)
```

### Criterion 3: Correlation
Removing features with more than 0.75 correlation
```{r, echo = FALSE}
# calculate correlation matrix
correlationMatrix <- round(abs(cor(data_2 %>% select(-c("Outcome")), use = "pairwise.complete.obs")),2)
plot <- ggplot(data = melt(correlationMatrix), aes(x=Var1, y=Var2, fill=value)) + 
        geom_tile() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
        xlab("Variables") + ylab(NULL)
plot

# save .png and latex code
fig_results(plot = plot, name = "correlation",
            caption = "Correlation Matrix for variables considered for the reduced feature set")

# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75, names = TRUE, exact = TRUE)
# print indexes of highly correlated attributes
cat("Highly correlated variable to be removed", highlyCorrelated)

data_reduced <- data_2 %>% select(-c(highlyCorrelated)) 
colnames(data_2)
cat("Reduced Feature Set with p<0.05 and <75% correlated\t", dim(data_reduced))
```


## Visualization
Plotting features selected considering univariate logistic regression pvalue results.

```{r, echo = FALSE}
plot_items <- function(var){
    
    plot1 <- ggplot(data_reduced, aes_string( x = var, fill = 'Outcome', width = 1)) + 
            geom_bar(position = "dodge") +
            theme(legend.position = c(0.84,0.88), text = element_text(size = 20)) +
            # scale_x_discrete("Rating", limits = c(1:6), breaks = c(1:6), labels = c(1:6)) +
            expand_limits(x = c(1,6))

    plot2 <- ggplot(data_reduced, aes_string( x = 'Outcome', y = var)) + 
            geom_violin(position = "dodge", width = 0.8, aes(color = Outcome, fill = Outcome), alpha = 0.2) +
            geom_boxplot(position = "dodge", aes(colour = Outcome), fill = "white",  width = 0.3) +
            theme(legend.position = "none", text = element_text(size = 20)) +
            expand_limits(y = c(1,6)) +
            ylab("Rating")
    plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO")

    title <- ggdraw() + 
        draw_label(gsub("[.]", " ", var), fontface = 'bold')
    
    plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))

    fig_results(plot = plot3,
                    name = gsub("[.]", "-", var),
                    caption = gsub("[.]", " ", var),
                    label = unlist(strsplit(var, "[.]"))[2])
    
    plot
}

lapply(colnames(data_reduced)[-length(colnames(data_reduced))], plot_items)

```

# Multivariate Model


## Multicollinearity 
The Variance Inflation Factor (VIF) was calculate to assess multicollinearity. 
VIF = 1 no correlation, 5 < VIF < 10 moderate correlation, VIF > 10 high multicollinearity. 
VIF larger than 5 or 10 is large and indicate a high level of multicollinearity and should be further processed.

```{r, echo = FALSE}
# including all features selected by lr test
model_lr_reduced <- glm(Outcome ~ ., data=data_reduced, family = binomial(link = logit)) 
summary(model_lr_reduced)
tab_results(data.frame(tidy(model_lr_reduced), row.names = 1),
            name = "multicollinearity",
            caption = "Multivariate model using the reduced feature set for multicollinearity analysis.")

# N = 47 samples do NOT contain NAs in 0.05 REDUCED dataset, M = 12 features
vif_reduced <- VIF(model_lr_reduced)
print(vif_reduced)
# save csv and print latex code
tab_results(data.frame(tidy(vif_reduced), row.names = 1),
            name = "VIF", 
            caption = "VIF for the reduced feature set")
```

## Preprocessing: Normalization + PCA
Only variables that had a pvalue < 0.05 in univariate logistic regression analysis. They were normalized and KNN imputation was used to replace missing values. 
Principal Component Analysis (PCA) was used to combine the original questionnaire items to derive features that capture the variance dataset and are uncorrelated (orthogonal).
The Principal Components that captured up to 95% of the variance of the original questionnaire data were used as predictors to estimate Outcome using Logistic Regression.

## Recursive Feature Elimination (RFE) + Multivariate Logistic Regression (LR)
Backwards Feature Selection is used to eliminate the least predictive Principal Components. 
It estimates the importance of each principal component by evaluating the difference in performance caused by the removal of individual components. 
Leave One Subject Out Cross Validation divides these datasets into train sets for modelling and test sets for estimating performance.
The AUC-ROC results achieved on the test sets by each of the models trained are considered when choosing which features to keep and remove in order to derive an optimal model.
This methodology was implemented in R using [rfe](https://search.r-project.org/CRAN/refmans/caret/html/rfe.html) (recursive feature elimination) function with ROC and [rfeControl](https://search.r-project.org/CRAN/refmans/caret/html/rfeControl.html) function with LOOCV in caret.

[Example with RF](https://towardsdatascience.com/effective-feature-selection-recursive-feature-elimination-using-r-148ff998e4f7), [ROC as metric](https://stackoverflow.com/questions/18242692/r-package-caret-rfe-function-how-to-customize-metric-to-use-auc).
```{r, echo = FALSE}
# GLM predicts the SECOND class -> make sure Fail is second class
# data_reduced$Outcome <- relevel(data_reduced$Outcome, ref = "Success")
# RFE calculates the metrics on the FIRST class -> make sure Fail is first class
data_reduced$Outcome <- relevel(data_reduced$Outcome, ref = "Fail")
cat('predicting second class:', levels(data_reduced$Outcome))

model_recipe <- recipe(Outcome ~ ., data = data_reduced )  %>%
        # step_impute_mean(all_numeric()) %>%
        step_normalize(all_numeric()) %>%
        step_pca(all_predictors(), threshold = 0.95)

# Setting ROC as the metric for the Logistic Regression function
# lrFuncs$summary <- twoClassSummary 
lrFuncs$summary <- prSummary 
set.seed(42)

ctrl <- rfeControl(functions = lrFuncs,         # Logistic Regression
                method = "cv",                  # Cross Validation
                number = nrow(data_reduced),   # Number of folds
                # method = "LOOCV",
                saveDetails = TRUE,
                returnResamp = "all",
                allowParallel = FALSE,
                rerank = TRUE,
                verbose = FALSE)

# Recursive Feature Elimination with feat_lr
model_rfe <- rfe(model_recipe,                 # predict Outcome using all other variables
                data = data_reduced,           # selecting the features from univariate lr
                sizes = 1:(ncol(data_reduced)-1), # from 1 to 10 variables
                rfeControl = ctrl,
                # metric = "Sens",               # optimising Specificity to avoid False Failures -> sens/spec are swapped in RFE
                metric = "Precision",
                maximize = TRUE)

warnings()
print(model_rfe)
# Estimate, z-value signs need to be swapped 
model <- summary(model_rfe$fit)
print(model)
```

Model fitness with deviance analysis. 
```{r, echo = FALSE}
# model fitness
dev = model$null.deviance - model$deviance
deg = model$df.null - model$df.residual
cat('\ndeviance difference: ', dev)
cat('\ndf difference: ', deg)
cat('\nlevel of significance: ', pchisq(dev, deg, lower.tail = FALSE))
cat('\nthe model is a good fit: ', pchisq(dev, deg, lower.tail = FALSE) < 0.05)
```

Performance metrics ROC and PR curves. 
```{r, echo = FALSE}
# cross validated predictions for performance analysis
df_perf <- model_rfe$pred %>% 
            filter(Variables == model_rfe$optsize) %>% 
            select(rowIndex, Fail, pred, obs) %>% 
            rename("Probability" = "Fail", "Predicted" = "pred", "Outcome" = "obs")

roc_curve <- roc.curve(scores.class0 = df_perf$Probability,
             weights.class0 = abs(as.numeric(df_perf$Outcome)-2),
             curve=TRUE)
plot(roc_curve)

png('results/dtq/roc-curve.png')
plot(roc_curve, xlab = "1 - Specificity", auc.main = FALSE, col = 'blue',
         main = paste('ROC Curve (AUC = ',toString(round(roc_curve$auc,2)),')',sep = ''), 
         cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
dev.off()

pr_curve <- pr.curve(scores.class0 = df_perf$Probability,
             weights.class0 = abs(as.numeric(df_perf$Outcome)-2),
             curve=TRUE)
plot(pr_curve)

png('results/dtq/pr-curve.png')
plot(pr_curve, auc.main = FALSE, col = 'blue',
         main = paste('PR Curve (AUC = ',toString(round(pr_curve$auc.integral,2)),')',sep = ''), 
         cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
dev.off()

# calculate performance metrics ROC
cm <- confusionMatrix(data = df_perf$Predicted, reference = df_perf$Outcome, positive = "Fail")
cm
metrics <- data.frame(Accuracy = cm$overall['Accuracy'],
                        ROC = roc_curve$auc,
                        PR =  pr_curve$auc.integral, 
                        t(cm$byClass))

metrics %>% round(3) %>% select(Accuracy, ROC, PR, Precision, Recall, Sensitivity, Specificity, F1, Balanced.Accuracy)

```

```{r, echo = FALSE}
# logit function, calculate y given the probability(x)
y <- function(x){ return(log(1/( 1/x - 1))) }
df_perf$Estimate <- y(df_perf$Probability)
df_perf$Status <- dtq$Status
levels(df_perf$Status)[levels(df_perf$Status) == "AD"] <- "SD"

th_fail = 0.7
th_success = 0.3

# releveling for the colours in the plot
df_perf$Outcome <- relevel(df_perf$Outcome, "Fail")
df_perf$Status <- relevel(df_perf$Status, "W")

print("Probability predicted by the LR using PRQ data vs true Outcome")
plot <- ggplot(df_perf, aes(x = Estimate, y = Probability, group = Outcome ))  +
    # bottom - green
    geom_rect(aes(xmin = -Inf, xmax = y(th_success), ymin = 0, ymax = th_success), 
            fill = "#99ff99", alpha = 0.2, color = "white") + 
    # top - orange
    geom_rect(aes(xmin = y(th_fail), xmax = Inf, ymin = th_fail, ymax = 1), 
            fill = "#ffff99", alpha = 0.2, color = "white") +  
    theme(legend.position = c(0.14,0.88), text = element_text(size = 20)) +
    geom_point(aes(shape=Outcome, color=Outcome), size = 5)
plot

fig_results(plot = plot, name = 'logit-outcome', 
            caption = "Probability predicted by the best logistic regression model using \acrshort{cbarq} data vs true Outcome")


print("Probability predicted by the LR using PRQ data vs true Status")
# plot Logit vs Status
plot <- ggplot(df_perf, aes(x = Estimate, y = Probability, color = Status )) +
    # bottom - green
    geom_rect(aes(xmin = -Inf, xmax = y(th_success), ymin = 0, ymax =  th_success), 
            fill = "#99ff99", alpha = 0.2, color = "white") + 
    # top - orange
    geom_rect(aes(xmin = y(th_fail), xmax = Inf, ymin =  th_fail, ymax = 1), 
            fill = "#ffff99", alpha = 0.2, color = "white") +  
    theme(legend.position = c(0.14,0.88), text = element_text(size = 20)) +
    geom_point(aes(shape=Status, color=Status), size = 5)
plot

fig_results(plot = plot, name = 'logit-status', 
            caption = "Probability predicted by the best logistic regression model using \acrshort{cbarq} data vs true Status")

print("Estimate and Probabilities per training outcome status")
df_perf %>% group_by(Status) %>% dplyr::summarize(Mean = mean(Estimate, na.rm=TRUE))
df_perf %>% group_by(Status) %>% dplyr::summarize(Mean = mean(Probability, na.rm=TRUE))

cat("Predicted probability <", th_success, " -> Green flag ")
print("Dogs flagged green are likely to SUCCEED, true outcome count and status proportion:")
table(df_perf %>% filter(Probability < th_success) %>% pull(Outcome))
prop.table(table(df_perf %>% filter(Probability < th_success) %>% pull(Status)))

cat("Predicted probability >", th_fail, "-> Yellow flag")
print("Dogs flagged yellow are likely to FAIL, true outcome count and status proportion:")
table(df_perf %>% filter(Probability > th_fail) %>% pull(Outcome))
prop.table(table(df_perf %>% filter(Probability > th_fail) %>% pull(Status)))

cat(th_success, " < Predicted probability <", th_fail, "-> no flags")
table(df_perf %>% filter(Probability > th_success & Probability < th_fail) %>% pull(Status))
```

## Interpretation

The best Logistic Regression model is used to investigate the effect of the principal components on the training outcome.
The estimates were exponentiated to get the Odds Ratio (OR). The 95% Confident Interval (CI) of the ORs was also calculated. 
RFE chose 1 principal components, PC1. 

```{r, echo = FALSE}
results_reduced <- as.data.frame(coef(summary(model_rfe$fit)))
results_reduced$OR <- exp(results_reduced$Estimate)
results_reduced <- cbind(results_reduced, exp(confint(model_rfe$fit, level = 0.95)))
print("Results from the model created with the reduced feature set")
print(results_reduced)

# save csv and print latex code
tab_results(results_reduced,
    caption = "Best logistic regression model using selected by RFE",
    name = "rfe-lr")
```

## Feature Importance

### Explained Variance

```{r, echo = FALSE}
# PCA variance
dc_pca_var <- as.data.frame(tidy(model_rfe$recipe, number = 2, type = "variance"))
dc_pca_var <- dc_pca_var %>% spread(key = "terms", value = value) %>% select(-c(id))
dc_pca_var$component <- mapvalues(dc_pca_var$component, from = 1:12, 
    to = c('PC01', 'PC02', 'PC03', 'PC04', 'PC05', 'PC06', 'PC07', 'PC08', 'PC09', 'PC10', 'PC11', 'PC12'))
colnames(dc_pca_var) <- make.names(names(dc_pca_var)) # fixing column names with space

print("Explained Variance per Principal Component (Scree Plot)")
plot <- ggplot(dc_pca_var, aes(x = component, y = percent.variance)) + #, fill = Included
    geom_bar(stat='identity', position = 'dodge', width = 0.8)+
    xlab("Principal components") +
    ylab("Explained Variance (%)") 
plot
suppressMessages(ggsave(paste("results/dtq/pc-explained-variance.png"), plot))

```

### Coefficient Loadings

```{r, echo = FALSE}
# model prep is the recipe steps, select pca -> number = 2, because it's the 3rd step
df_pca_coef <- as.data.frame(tidy(model_rfe$recipe, number = 2, type = "coef"))  %>% 
                filter(component == 'PC1') %>% select(terms, value) 

# if I ever want to get rid of the . in the x axis labels
# gsub("[.]" , " ",df_pca_coef$term[1])

plot <- ggplot(df_pca_coef, aes(x=terms, y=value)) +
    geom_bar(stat='identity', position='dodge', width = 0.8) +
    xlab("MCPQ-R items - Predictor Variables") +
    ylab("Principal components") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
plot    

fig_results(plot = plot, name = 'pc-loading',
            caption = "PC1 coefficient loading from original items")

# save csv and print latex code
tab_results(df_pca_coef %>% remove_rownames %>% column_to_rownames(var="terms"), 
            name = 'pc-loading', caption = 'PC1 coefficient loading from original MCPQ-R items')

# figuring out the righest loading items for the PC1
df_pca_coef %>% arrange(desc(abs(value))) %>% select(terms, value)
```

# Conclusion
Completing this questionnaire during training (Week 10) would allow assistance dog training organisations to understand which dog are more suitable and allow them to make informed decisions when analysing which dogs to keep for training considering the results of this objective assessment. 
