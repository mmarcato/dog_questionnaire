# New model: Recipes + Caret RFE + CV (number = nrows)
recipes + caret rfe  -> I do NOT understand why this is not working, 


I tried giving the recipe for preprocessing to the rfe function, but it did not work with LOOCV for some reason!
I did some research (keywords: Caret rfeControl with loocv Num_Resamples) but there is not much out there.
I managed to make it run, but I needed to change the method to be 'cv' with 63 folds 
PRQ
I ran the models with df = data_0.05_full only because it gave the best results
Best model PC01            ROC = 0.8712 (with repeats = 2, size = 1:3)
Best model PC01, PC05       ROC = 0.8394 (with repeats = 5, size = 1:3)
Best model PC01, PC05       ROC = 0.8394 (with repeats = 5, size = 1:10)

DTQ
dataset method          ROC     Sens    Spec    ROCSD   SensSD  SpecSD  Num_Resamples Selected
p<0.05  repeatedCV      0.8843  0.9667  0.4167  0.2450  0.1214  0.5000            50        *


The only problem is that it won't calculate the ROC
idx <- 1:63
idxs <- list()
for (i in 1:63) {idxs[i] <- list(idx[-i])}

if using trControl

# I could calculate several performance metrics on crossvalidation
MySummary  <- function(data, lev = NULL, model = NULL){
  a1 <- defaultSummary(data, lev, model)
  b1 <- twoClassSummary(data, lev, model)
  c1 <- prSummary(data, lev, model)
  out <- c(a1, b1, c1)
  out}
trControl = trainControl(
        savePredictions = FALSE,
        classProbs = TRUE, 
        summaryFunction = MySummary
        ),

I included this in rfecontrol
index = idxs,
indexOut = NULL,

```{r, echo = FALSE}
colnames(data_reduced)
df = data_reduced

model_recipe <- recipe(Outcome ~ ., data = df )  %>%
        step_normalize(all_numeric()) %>%
        step_impute_mean(all_numeric()) %>%
        step_pca(all_predictors(), threshold = .95)

# Setting ROC as the metric for the Logistic Regression function
lrFuncs$summary <- twoClassSummary
set.seed(42)

ctrl <- rfeControl(functions = lrFuncs,         # Logistic Regression

                    # method = "LOOCV",         # Leave One Out Cross Validation

                    method = "cv",              # Cross Validation
                    number = 63,                # Number of folds

                    # method = "repeatedcv",    # Repeated Cross Validation
                    # number = 63,              # Number of folds
                    # repeats = 1,              # Number of iterations/repetition

                    saveDetails = TRUE,
                    returnResamp = "final",
                    rerank = TRUE,
                    verbose = FALSE)
                    
# Recursive Feature Elimination with feat_lr
model_rfe <- rfe(model_recipe,                 # predict Outcome using all other variables
                data = df,                      # selecting the features from univariate lr
                sizes = c(1:10),                # from 1 to 10 variables
                rfeControl = ctrl,
                metric = "ROC",                # optmising specificity
                maximize = TRUE,
                preProc = c("center", "scale")# apparently rfe can also preprocess data? I didn't try this!
                )

warnings()
print(model_rfe)
print(summary(model_rfe$fit))
df_pred <- model_rfe$pred %>% filter(Variables == 1) 

# making sure it is always getting a different example for validation
table(df_pred$Resample)                           # number of samples per fold
df_pred %>% select(obs, rowIndex)                 # index of the test 
data_reduced$Outcome
attributes(model_rfe)

# calculate performance metrics
confusionMatrix(data = df_pred$pred, reference = df_pred$obs, positive='Success')
# confusionMatrix(data = df_pred$pred, reference = df_pred$obs, mode = "prec_recall")

# calculate the roc
roc(response = as.numeric(relevel(df_pred$obs, "Success")),        # true class
        predictor = as.numeric(relevel(df_pred$pred, "Success")),  # predicted class 
        plot = TRUE, levels = c(1, 2))

ggplot(data = model_rfe, metric = "Spec") + theme_bw()
print("Features in the model that delivered the best ROC")
print(predictors(rfe_lr))

summary(model_rfe$fit)

# PCA variance
as.data.frame(tidy(model_rfe$recipe, number = 3, type = "variance"))
# PCA coefficients
as.data.frame(tidy(model_rfe$recipe, number = 3, type = "coef"))

```

# Previous model: Recipes -> Caret RFE + LOOCV (number = nrows)
Issues: using LOOCV in rfeControl does not allow me to
1. Use recipes -> Data leakage as PCA is performed before RFE insted of inside the loop.
2. Access crossvalidations predictions -> I neede to use the model to predict on the original dataset.

<!-- 
I did not manage to make the preprocessing work within rfe using LOOCV, it only works with repeatedCV.
Therefore, I had to implement the preprocessing steps (normalization, imputation and pca) using recipes steps and then 
use that transformed dataframe to learn using RFE + LR.
This results in dataleakage in the ROC estimations using LOOCV.
The resulting models using LOOCV and repeatedCV had PC01 and sometimes PC05 and ROC varying between 0.80 and 0.87, depending on the number of repeats.  -->

Interpreting performance metrics:

- Sensitivity (TPR) = positive results for fail dogs. How many withdrawn dogs were flaged as fail?
- Type 1 error = false positive rate: 1-TPR = 8% of the dogs flagged as fail would successed.

- Specificity (TNR) = negative result for success dogs. How many graduate dogs were flaged as success?
- Type 2 error = false negative error: 1-TNR = 64% of the dogs flagged as success would fail.
<!-- 
The results reveal that the model learnt on the reduced dataset delivers a better AUC-ROC 0.81 compared to 0.75.
On the other hand, the TPR or Sensitivity is 98% for the Reduced model and 92% for the Full model. 
Therefore, the Reduced model is a better fit for the data. 
The sample size is rather small, therefore that such differences performance may be caused for just a few dogs being correctly classified.
Regardless, a simpler model is always a better choice if the differences in performance are not significant.
Another metric that can be used to compare model is AIC (Akaike Information Criterion), an optimal model minimizes AIC.

Conclusively, the AIC indicates that the Reduced model is a better fit. -->

```{r, echo = FALSE}
impute_pca <- function(df){

    print("Dimensions of original dataset:")
    print(dim(df))

    model_pca <- recipe(formula =  ~ ., data = df) %>%
                    step_normalize(all_numeric()) %>%
                    step_impute_knn(all_numeric(), neighbors = 3) %>%
                    step_pca(all_numeric(), threshold = .95)

    model_prep <- prep(model_pca, training = df)
    return(model_prep)
}

rfe_lr <- function(df){
#     df = data_pca
    # Setting ROC as the metric for the Logistic Regression function
    lrFuncs$summary <- twoClassSummary
    set.seed(77)

    ctrl <- rfeControl(functions = lrFuncs,     # logistic regression
                        method = "LOOCV",       # Leave One Out Cross Validation
                        saveDetails = TRUE,
                        returnResamp = "final",
                        rerank = TRUE,
                        verbose = FALSE)

    # Recursive Feature Elimination with feat_lr
    model <- rfe(Outcome ~ . ,                  # predict Outcome using all other variables
                    data = df,                  # selecting the features from univariate lr
                    sizes = c(1:length(df)),
                    rfeControl = ctrl,
                    metric = "ROC")
        
    print(model$pred)

    # Printing outputs
    warnings()
    
    sink(file = "results/prq/rfe_output.txt")
    print(model)
    sink(file = NULL)

    print(summary(model$fit))
    
    return(model)
}

dev_null_residual <- function(model){
    dev = model$null.deviance - model$deviance
    deg = model$df.null - model$df.residual
    cat('\ndeviance difference: ', dev)
    cat('\ndf difference: ', deg)
    cat('\nlevel of significance: ', pchisq(dev, deg, lower.tail = FALSE))
    cat('\nthe model is a good fit: ', pchisq(dev, deg, lower.tail = FALSE) < 0.05)
}

```

```{r, echo = FALSE}
data_reduced$Outcome
data_prep <- impute_pca(data_reduced)
data_pca  <- bake(data_prep, data_reduced)
rfe_reduced <- rfe_lr(data_pca)
rfe_reduced

dev_null_residual(summary(rfe_reduced$fit))

# save csv and print latex code
tab_results(as.data.frame(rfe_reduced$results), name = "rfe-results",
    caption = "Recursive Feature Selection with Leave-One-Out Cross Validation"  )

# plot roc per rfe iteration
plot <- ggplot(data = rfe_reduced, metric = "ROC") + theme_bw() +
            expand_limits(y = c(0,1))
plot

fig_results(plot = plot, name = "rfe-roc",
            caption = "Recursive Feature Elimination ROC achieved considering Leave-One-Out Cross-Validation for each model versus the number of variables included in each model")
            
```

# Caret trainControl + train
```{r, echo = FALSE}

# train control for preprocessing dataset
MyTrainControl <- trainControl(
                method = "LOOCV", 
                classProbs = TRUE, # compute the class probability (true enables ROC)
		        # preProcOptions = list(method = c('center', 'scale', 'knnImpute', 'pca'), k = 3, thresh = 0.95),
		        preProcOptions = list('center', 'scale', 'knnImpute', 'pca', k = 3, 'thresh' = 0.95),
                # PCAthresh =0.95, k = 3,
                savePredictions= "final",
			    summaryFunction = twoClassSummary) # calculate ROC metric

model <- train(Outcome ~ ., data = df,
              method = lrFuncs,
              metric = "ROC",
              trControl = MyTrainControl)

```


# Caret RFE + trainControl + rfeControl
RFE model in R. Apparently RFE allows you to specify pre-processing (link)[https://stackoverflow.com/questions/44776763/recursive-feature-elimination-with-caret-metric-roc-is-not-created-by-the-sum]. 
I could use the trControl = (trainControl)[https://www.rdocumentation.org/packages/caret/versions/6.0-92/topics/trainControl] and specify preProcOptions (link)[https://www.rdocumentation.org/packages/caret/versions/6.0-92/topics/preProcess]
Using (trainControl)[https://stackoverflow.com/questions/24612824/r-caret-package-error-if-i-specified-index-for-both-rfe-control-and-train-contr]
Ignoring (trainControl)[https://stackoverflow.com/questions/54088406/r-caret-combine-rfe-and-train]
```{r, echo = FALSE}
# df = data_lr
# Setting ROC as the metric for the Logistic Regression function
lrFuncs$summary <- twoClassSummary

# I saw somewhere that I should try using seeds, but I think this does not apply for LOOCV
set.seed(36)
seeds <- vector(mode = "list", length = 51)
for(i in 1:50) seeds[[i]] <- sample.int(1000, length(subsetSizes) + 1)
seeds[[51]] <- sample.int(1000, 1)

# train control for preprocessing dataset
MyTrainControl <- trainControl(
                method = "LOOCV", 
                classProbs = TRUE, # compute the class probability (true enables ROC)
		        # preProcOptions = list(method = c('center', 'scale', 'knnImpute', 'pca'), k = 3, thresh = 0.95),
		        preProcOptions = list('center', 'scale', 'knnImpute', 'pca', k = 3, 'thresh' = 0.95),
                # PCAthresh =0.95, k = 3,
                savePredictions="final",
			    summaryFunction = twoClassSummary # calculate ROC metric
                )

MyRFEcontrol <- rfeControl(functions = lrFuncs,     # logistic regression
                    # method = "LOOCV",       # Leave One Out Cross Validation
                    # number = 1, 
                    method = "repeatedcv",  # Repeated Cross Validation
                    number = 4,             # Number of folds 
                    repeats = 10,           # Number of iterations/repetition
                    verbose = FALSE,
                    returnResamp = "all")

# Recursive Feature Elimination with feat_lr
model <- rfe(Outcome ~ . ,                     # predict Outcome using all other variables
                data = df,                  # selecting the features from univariate lr
                sizes = 1:12, 
                number = 63,                
                # Num_Resamples = 2,
                trControl = MyTrainControl,
                rfeControl = MyRFEcontrol,
                metric = "Sens",
                maximize = TRUE) 


```
