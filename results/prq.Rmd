---
title: "Puppy Raiser Questionnaire vs Outcome (Success, Fail)"
author: "Marinara Marcato"
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/marinara.marcato/Project/Scripts/dog_questionnaires")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE) 

# install.packages("broom")
library(DescTools)
# plot
library(ggplot2)
library(ggpubr)
library(cowplot)

# datasets
library(plyr)
library(dplyr)
library(tidyverse)
library(reshape2)

library(ggsci)  # colors for the graphs - npg for nature
library(broom)  # convert r output into tibbles
library(xtable) # latex table output

# machine learning/stats
library(mlbench)
library(caret)
library(recipes)
```

```{r output, include = FALSE, results = "hide", message = FALSE}
# save dataframes to csv and print LaTeX table
tab_results <- function(df, name, caption=NULL) {

    # remove the dots from the rownames
    rownames(df) <- gsub("[.]", " ", rownames(df))
    
    path = paste("results/prq/", name, ".csv", sep = "")

    # save csv file
    write.csv(df, path)
    cat("dataframe saved to ", path)
    
    # latex code for table
    # print(xtable(df,
    #     caption = paste(caption, ".", sep = ""),
    #     label = paste("T-prq-", name, sep = "")),
    #     caption.placement = "top")
}

# save images as png and print LaTeX table
fig_results <- function(plot, name, caption = NULL, label = NULL){

    path = paste("results/prq/", name, ".png", sep = "")
    
    # if label is not given, use name
    if (is.null(label)){
        label = name
    }
    # save png
    suppressMessages(ggsave(filename = path, 
                plot = plot + theme(text = element_text(size = 20)),
                width = 12, height = 8))
    cat("Image saved at", path)

    # latex code for image
    # cat("\n\nStart LaTeX code\n\n",
    #         paste("\\begin{figure}[!h]\n\\centering\n\\caption{", paste(caption, ".", sep = ""),"}","\n\\label{F-prq-", label, "}\n\\includegraphics[width = 11cm]{",path,"}\n\\end{figure}", sep = ""), 
    #         "\n\nEnd LaTeX Code\n\n")
}
```

# Introduction 
In order to objectively evaluate trainee guide dogs personality, their puppy raisers filled out the standardised questionnaire Canine Behavioural Assessement & Research Questionnaire (C-BARQ) around the time they were due to start formal training.
This document shows the data analysis carried out to investigate the association between ratings given by the puppy raisers and the dog's training outcome.
Statistical methods will be used to test the hypothesis that there is a relationship between personality ratings and training outcome (Success, Fail).

# Data Exploration

Importing data and converting variables to adequate data types.
```{r, echo = FALSE}
prq = read.csv('data/2022-08-08-PRQ_C-BARQ.csv', stringsAsFactors=TRUE)
# colnames(prq)
# converting date types
prq$Timestamp = as.Date(prq$Timestamp, format= "%Y-%m-%d")
prq$DOB = as.Date(prq$DOB, format= "%Y-%m-%d")
prq$DOA = as.Date(prq$DOA, format= "%Y-%m-%d")
prq$End.Date = as.Date(prq$End.Date, format= "%Y-%m-%d")
prq$Duration = as.numeric(gsub(" .*$", "", prq$Duration))
```

## Demographics

The number of dogs in the Puppy Raiser Questionnaire dataset. Selection criteria: dogs who successed training (assistance and guide dogs) and dogs withdrawn for behavioural reasons.
```{r, echo = FALSE}
cat('Number of dogs:', length(prq$Code))
cat('Training Outcome:')
table(prq$Outcome)
```

Analysing categorical demographic data: Sex, Breed. There was only one German Shepherd and Golden Doodle dogs in the sample, their breeds were relabeled as "Other" for the data analysis.
```{r, echo = FALSE}
print(table(prq$Sex))
print("Original classes")
table(prq$Breed)
# merging breed categories
levels(prq$Breed)[levels(prq$Breed) == "LRx"] <- "LRxGR"
levels(prq$Breed)[levels(prq$Breed) =="GS" | levels(prq$Breed) =="GRxPoodle"] <- "Other"
print("Processed classes")
table(prq$Breed)
```

Analysing age at arrival to the training centre and age at assessment when the questionnaire was completed:
```{r, echo = FALSE}
n <- dim(prq)[1]

prq$Age.at.Arrival <- prq$DOA - prq$DOB
mean <- mean(prq$Age.at.Arrival)
std <- sd(prq$Age.at.Arrival)
margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)

cat('Age at Arrival: Mean', round(mean/30.417, 2), 
            'Standard Deviation', round(std/30.417, 2))
            # 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)


prq$Age.at.Assessment <- prq$Timestamp - prq$DOB
mean <- mean(prq$Age.at.Assessment)
std <- sd(prq$Age.at.Assessment)
margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)

cat('Age at Assessment: Mean', round(mean/30.417, 2), 
            'Standard Deviation', round(std/30.417, 2))
            # 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)
```

Calculate statistics of duration of training for the dogs that were withdrawn from training. 
```{r, echo = FALSE}
# Duration of training before withdrawal in weeks
duration <- prq %>% filter(Outcome == "Fail") %>% select(Duration)/7
print('Duration of Training in weeks')
summary(duration)

h <- ggplot(duration, aes(x=Duration)) +
 geom_histogram(binwidth = 1) +
 xlab("Duration (Weeks)") +
 ylab("Number of Dogs") + theme_bw()

h

fig_results(h, name = "duration-histogram",
            caption = "Duration of training in weeks for dogs that were withdrawn from training for behavioural reasons.")
```

## Descriptive Statistics
The C-BARQ contains 100 items which were scored by the Puppy Raisers in a scale from 'Never', 'Seldom', 'Sometimes', 'Usually', 'Always' and encoded in a scale from 0 to 4. 
Out of the 100 items, 78 items are used to calculate 14 factors and the remaining 22 items are myscellaneous.
The questionnaire data are kept as numeric, rather than being converted to factors, so the information about the ordering is kept.
The descriptive statistics of the questionnaire data shown below are calculated and saved as a csv.  
Missing data is also analysed. 

```{r, echo = FALSE}
data = prq %>% select(starts_with(c('I_', 'F_')), Sex, Breed, Outcome)
# LR will predict the probability of the LAST class 
# R defaults to alphabetical order, therefore, between [Fail Success], it will predict Success.
# releveling the independent variable so we will predict Fail instead of Success
data$Outcome <- relevel(data$Outcome, "Success")
print("Items")
colnames(data[1:100])
print("Factors, Demographic and Outcome")
str(data[101:117])
# FACTOR CALCULATIONS  - Items used 1–76 and 91–92; Items NOT used 77-90 and 93-100 
    # 1.'Stranger-directed aggression' score = (questionnaire items 10 + 11 + 12 + 15 + 16 + 18 + 20 + 21 + 22 + 28)/10.
    # 2.'Owner-directed aggression' score = (items 9 + 13 + 14 + 17 + 19 + 25 + 30 + 31)/8.
    # 3.'Dog-directed aggression' = (items 23 + 24 + 26 + 29)/4
    # 4.'Dog-directed fear' = (items 45 + 46 + 52 + 53)/4
    # 5.'Dog rivalry'(familiar dog aggression) score = (items 32 + 33 + 34 + 35)/4
    # 6.'Trainability' score = (items 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8)/8—remember to reverse scoring order for items 5, 6 & 7 (see above).   
    # 7.'Chasing' score = (items 27 + 74 + 75 + 76)/4
    # 8.'Stranger-directed fear' score = (items 36 + 37 + 39 + 40)/4
    # 9.'Nonsocial fear' score = (items 38 + 41 + 42 + 44 + 47 + 48)/6
    # 10.'Separation-related problems' score = (items 54 + 55 + 56 + 57 + 58 + 59 + 60 + 61)/8
    # 11.'Touch sensitivity' score = (items 43 + 49 + 50 + 51)/4
    # 12.'Excitability' score = (items 62 + 63 + 64 + 65 + 66 + 67)/6
    # 13.'Attachment/attention-seeking' score = (items  68 + 69 + 70 + 71 + 72 + 73)/6
    # 14.'Energy' score = (items 91 + 92)/2
```

```{r, echo = FALSE}
# calculate descriptive statistics
stats <- data.frame(do.call(rbind, lapply(data[1:114], summary)))

# missing values, there is something weird about the NAs calculations using summary
stats$NA.s <- colSums(is.na(data[1:114]))
print('Features sorted by number of missing rows')
print(stats[order(stats$NA.s, decreasing = TRUE)[1:18], 'NA.s', drop = FALSE])

# histogram of missing values in original dataset
h <- ggplot(stats, aes(x=NA.s)) +
    geom_histogram(binwidth = 1) +
    xlab("Number of Missing values ") +
    ylab("Number of Variables") + theme_bw()

# 28 NAs -> Item 49 nails clipped by someone in the house
# 12-10 NAs -> Because of no other dog  in household -> Items 33,34,32,35

# save csv and print latex code
tab_results(stats, name = "descriptive-statistics")
```

## Feature Selection
### Criteria 1, 2 and 3: Univariate Logistic Regression
Logistic regression was used to investigate whether there was a statistically significant difference between the behaviours reported by Puppy Raisers and the Training Outcome of the dogs.
C-BARQ Correlation with Training Outcome. 

Breeds (p = 0.44 and p = 0.16) and Sex (p = 0.61) were not significantly associated with training outcome.
```{r, echo = FALSE}
# univariate logistic regression using one predictor and outcome
lr_models <- lapply(data[-length(data)], 
                        function(x) glm(formula = Outcome ~ x, 
                        data = data, 
                        family = binomial(link = logit), na.action = na.exclude))
lr_result <- lapply(lr_models, function(x) c(coef(summary(x)), summary(x)$deviance))

## univariate models with 3 dfs (Breed)
lr_breeds <- data.frame(transpose(lr_result[116]))
colnames(lr_breeds) <- c("estimate_0", "estimate_1", "estimate_2", 
                    "se_0", "se_1", "se_2", "z_value_0", "z_value_1", "z_value_2", 
                    "p_value_0", "p_value_1", "p_value_2", "deviance")
print(lr_models[116])
cat('Difference between Other and LR: \t', lr_breeds$p_value_1, '\t -> not significant')
cat('Difference between Other and LRxGR: \t', lr_breeds$p_value_2, '\t -> not significant')

## univariate models with up to 2 dfs (items are ints)
lr_results <- data.frame(do.call(rbind,lr_result[1:115]))
colnames(lr_results) <- c("estimate_0", "estimate_1", 
                    "se_0", "se_1", "z_value_0", "z_value_1",
                    "p_value_0", "p_value_1", "deviance")

## univariate models with 1 df (because the predictor variable is constant)
print("Columns 9,10,13,17,30,37 are CONSTANT, that's why they p_values don't make sense")
lr_results[c(9,10,13,17,30,37),'p_value_1']

# save csv and print latex code
tab_results(lr_results, name = "univariate-logistic-regression")
``` 

Converting logistic regression result to odds ration and confidence interavals. 
```{r, echo = FALSE}
lr <- lr_results %>% select(estimate_1, p_value_1) # se_1, z_value_1,
colnames(lr) <- c("Estimate", "p-value")
lr$OR <- exp(lr$Estimate)
# list of dataframes of CI for each of the models on the list
lr_cis <- lapply(lr_models[-length(lr_models)], function(x) as.data.frame(exp(confint(x, level = 0.95))))
# list of dataframes removing the CI for the intercept
lr_cis <- lapply(lr_cis, function(x) x[2,])
# turn list of dataframes into a dataframe
lr_ci <- bind_rows(lr_cis, .id = 'x')
# replace rownames with column with the name of the variables
lr_ci <- lr_ci %>% remove_rownames %>% column_to_rownames(var="x")

# bind CIs
lr <- cbind(lr, lr_ci)

# save csv and print latex code
tab_results(lr,
    caption = "Univariate logistic regression model estimates, p-value, odds ratio (OR) and confidence interval (CI)",
    name = "lr-or-ci")
```

Analysing p-values and deviance.
Analysing deviance of full model compared to the null model. 
Reduction in deviance must be significant (chi-squared test) for the model to be considered a good fit.
```{r, echo = FALSE}
# CRITERION 1: pvalue analysis
print("Number of features considering each level of significance using Logistic Regression p-value & difference in deviance test")
lr_results$significant_0.05 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.05) 
lr_results$significant_0.1 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.1) 
lr_results$significant_0.2 <- (lr_results$p_value_1 > 0) & (lr_results$p_value_1 < 0.2) 
print(colSums(lr_results[,10:12], na.rm = TRUE))
# inclusion of X96,98 and F3,6 in feature set p<0.2 compared to with p<0.05 

# CRITERION 2: deviance analysis (Null deviance: 58.352  on 62  degrees of freedom = N-1)
lr_results$deviance_diff <- pchisq((58.352 - lr_results$deviance), 1, lower.tail = FALSE)
lr_results$deviance_sign <- (lr_results$deviance_diff < 0.05)
# significant differences in deviances
cat('Number of features whose univariate models resulted in a significant difference in deviance: ', sum(lr_results$deviance_sign))

```

Feature selection based on the univariate logistic regression p-value and deviance analysis. 
This reduced feature set contains all factors and only items that were NOT included in the calculation of statistically significant factor with p<0.05 (i.e. Items 1,27 an 75 and some myscellaneous items 77-90, 93-100).
```{r, echo = FALSE}
# CRITERIA 1 and 2
print("All features with level of significance < 0.05 using Univariate Logistic Regression p-value test")
data_2 <- data %>% 
    select(all_of(lr_results %>% filter(significant_0.05 ==TRUE & deviance_sign == TRUE) %>% rownames()), Outcome)
cat("Full Feature Set with p<0.05 and reduction in deviance\t", dim(data_2))

# CRITERION 3
# items that were significant but not used to calculate significant factors 1, 27, 75 -> "I_01", 27, 75,
# items not used to calculate any factors 77-90 and 93-100
print("Keeping items 1, 27, 75 and anything between 77-90 and 93-100")
data_3 <- data_2  %>% 
        select(contains(c("I_01", "I_27", "I_75", as.character(c(77:90, 93:100)), 'Sex', 'Breed', 'F_')), Outcome)
cat("Reduced Feature Set with p<0.05\t", dim(data_3))
colnames(data_2)
```

### Criterion 4: Missing Data
PRQ, missing values per column and per row. Removing features with more than 5% missing data (i.e. I_77, I_93).
```{r, echo = FALSE}
# reduced dataset
NA.s <- unlist(lapply(data_3, function (x) sum(is.na(x))))
NA.s
# save csv and print latex code
tab_results(data.frame(NA.s), name = "missing",
        caption = "Variables considered for the reduced feature set and number of missing values.")
print(which(NA.s > 5))
# removing 93 and 77 contains as they contained >5% data missing
data_4 <- data_3  %>% select(-contains(as.character(c(77, 93))))
cat("Reduced Feature Set with p<0.05 and < 5% missing data\t", dim(data_4))
```

### Criterion 5: Correlation
Removing features with more than 0.75 correlation (i.e. F_03)
```{r, echo = FALSE}
# change col names for the plot
colnames(data_4) <- unlist(lapply(strsplit(colnames(data_4),"[.]"), function(x) x[1]))

# calculate correlation matrix
correlationMatrix <- round(abs(cor(data_4 %>% select(-c("Outcome")), use = "pairwise.complete.obs")),2)
plot <- ggplot(data = melt(correlationMatrix), aes(x=Var1, y=Var2, fill=value)) + 
        geom_tile() + theme(axis.text.x = element_text(angle = 45)) +
        xlab("Variables") + ylab(NULL)
plot

# save .png and latex code
fig_results(plot = plot, name = "correlation",
            caption = "Correlation Matrix for variables considered for the reduced feature set")

# find attributes that are highly corrected (ideally > 0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75, names = TRUE, exact = TRUE)
# print indexes of highly correlated attributes
cat("Highly correlated variable to be removed", highlyCorrelated)

data_reduced <- data_3  %>% select(-contains(as.character(c(77, 93, highlyCorrelated))))
cat("Reduced Feature Set with p<0.05 and < 5% missing data and <75% correlated\t", dim(data_reduced))
```


### Visualization
Plotting the items and factors that will be used for modelling.
```{r, echo = FALSE}
plot_discrete <- function(var){
    
    plot1 <- ggplot(data_reduced, aes_string( x = var, fill = 'Outcome', width = 1)) + 
            geom_bar(position = "dodge") + 
            theme(legend.position = c(0.84,0.88), text = element_text(size = 20)) +
            # scale_x_discrete("Rating", limits = c(0:4), breaks = c(0:4), labels = c(0:4)) +
            expand_limits(x = c(0,4))

    plot2 <- ggplot(data_reduced, aes_string( x = 'Outcome', y = var)) + 
            geom_violin(position = "dodge", width = 0.8, aes(color = Outcome, fill = Outcome), alpha = 0.2) +
            geom_boxplot(position = "dodge", aes(colour = Outcome), fill = "white",  width = 0.3) +
            theme(legend.position = "none", text = element_text(size = 20)) +
            expand_limits(y = c(0,4)) +
            ylab("Rating")

    plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO") 
    title <- ggdraw() + 
            draw_label(gsub("[.]", " ", var), fontface = 'bold')
    
    plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))

    fig_results(plot = plot3,
                    name = gsub("[.]", "-", var),
                    caption = gsub("[.]", " ", var),
                    label = unlist(strsplit(var, "[.]"))[1])
    
    plot
}

feat_items <- data_reduced %>% select(-Outcome, -contains('F_')) %>% colnames
lapply(feat_items, plot_discrete)

plot_continuous <- function(var){

    # bin = (max(data_reduced[var], na.rm = TRUE) - min(data_reduced[var], na.rm = TRUE)) / 5
    max_x <- max(data_reduced[var], na.rm = TRUE)
    min_x <- min(data_reduced[var], na.rm = TRUE)
    bin <- seq(min_x, max_x, (max_x - min_x)/10)
    # print(bin)
    
    plot1 <- ggplot(data_reduced, aes_string(x = var, fill = 'Outcome')) + 
            geom_histogram(position = "dodge", breaks = bin) +
            theme(legend.position = c(0.84,0.88), text = element_text(size = 20))

    plot2 <- ggplot(data_reduced, aes_string( x = 'Outcome', y = var)) + 
            geom_violin(position = "dodge", width = 0.8, aes(color = Outcome, fill = Outcome), alpha = 0.2) +
            geom_boxplot(position = "dodge", aes(colour = Outcome), fill = "white",  width = 0.3) +
            theme(legend.position = "none",text = element_text(size = 20)) +
            ylab("Rating")

    plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO")

    title <- ggdraw() + 
            draw_label(gsub("S[.]|[.]", " ", var), fontface = 'bold')
    
    plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))

    fig_results(plot = plot3, 
                    name = gsub("[.]", "-", var),
                    caption = gsub("[.]", " ", var),
                    label = unlist(strsplit(var, "[.]"))[1])

    plot
}   

feat_factors <- data_reduced %>% select(contains('F_')) %>% colnames
lapply(feat_factors, plot_continuous)

```


# Multivariate Model

## Multicollinearity 
Calculate Variance Inflation Factor (VIF) to avoid multicollinearity. 
The glm models were built on the reduced feature sets and exclude any sample containing NAs.
VIF larger than 5 or 10 is large and indicate a high level of multicollinearity.

The resulting multivariate logistic regression models are NOT looking good, p_values are all roughly 1.
Mainly because they have small sample size (examples with NAs were removed).
VIF values indicate a high degree of multicollinearity. 

```{r, echo = FALSE}
cat("VIF reduced")
model_lr_reduced <- glm(Outcome ~ ., data=data_reduced, family = binomial(link = logit), na.action = na.exclude)
summary(model_lr_reduced)
tab_results(data.frame(tidy(model_lr_reduced), row.names = 1),
            name = "multicollinearity",
            caption = "Multivariate model using the reduced feature set for multicollinearity analysis.")

# N = 47 samples do NOT contain NAs in 0.05 REDUCED dataset, M = 12 features
vif_reduced <- VIF(model_lr_reduced)
print(vif_reduced)
# save csv and print latex code
tab_results(data.frame(tidy(vif_reduced), row.names = 1),
            name = "VIF", 
            caption = "VIF for the reduced feature set")
```

## Preprocessing: Normalization + Imputation + PCA
PCA was performed using the entire reduced feature set. 
The "recipes" package was used to perform preprocessing (missing value imputation + normalization + PCA).
[link](https://recipes.tidymodels.org/reference/step_pca.html),
[Recipes documentation](https://cran.r-project.org/web/packages/recipes/recipes.pdf),
[Example Recipes](https://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)

## Recursive Feature Elimination (RFE) + Multivariate Logistic Regression (LR)
Feature selection was performed using the caret package's rfe function.
Logistic Regression was used to model the dataset and Leave One Subject Out Cross Validation was employed to validate the results.
[Caret RFE](https://search.r-project.org/CRAN/refmans/caret/html/rfe.html), 
[Caret RFE Control](https://search.r-project.org/CRAN/refmans/caret/html/rfeControl.html), 
[Examples using RFE with Recipes](http://topepo.github.io/caret/recursive-feature-elimination.html#rferecipes)
[Example RFE + RF as predictor](https://towardsdatascience.com/effective-feature-selection-recursive-feature-elimination-using-r-148ff998e4f7), 
[RFE + ROC as metric](https://stackoverflow.com/questions/18242692/r-package-caret-rfe-function-how-to-customize-metric-to-use-auc).
<!-- 
I did not manage to make the preprocessing work within rfe using LOOCV, it only works with repeatedCV.
Therefore, I had to implement the preprocessing steps (normalization, imputation and pca) using recipes steps and then 
use that transformed dataframe to learn using RFE + LR.
This results in dataleakage in the ROC estimations using LOOCV.
The resulting models using LOOCV and repeatedCV had PC01 and sometimes PC05 and ROC varying between 0.80 and 0.87, depending on the number of repeats.  -->

```{r definition, echo = FALSE}
impute_pca <- function(df){

    print("Dimensions of original dataset:")
    print(dim(df))

    model_pca <- recipe(formula =  ~ ., data = df) %>%
                    step_normalize(all_numeric()) %>%
                    step_impute_knn(all_numeric(), neighbors = 3) %>%
                    step_pca(all_numeric(), threshold = .95)

    model_prep <- prep(model_pca, training = df)
    return(model_prep)
}

rfe_lr <- function(df){

    # Setting ROC as the metric for the Logistic Regression function
    lrFuncs$summary <- twoClassSummary
    set.seed(77)

    ctrl <- rfeControl(functions = lrFuncs,     # logistic regression
                        rerank = TRUE,
                        method = "LOOCV",       # Leave One Out Cross Validation
                        verbose = FALSE)

    # Recursive Feature Elimination with feat_lr
    model <- rfe(Outcome ~ . ,                  # predict Outcome using all other variables
                    data = df,                  # selecting the features from univariate lr
                    sizes = c(1:length(df)),
                    rfeControl = ctrl,
                    metric = "ROC") 
    
    # Printing outputs
    warnings()
    
    sink(file = "results/prq/rfe_output.txt")
    print(model)
    sink(file = NULL)

    print(summary(model$fit))
    
    return(model)
}

dev_null_residual <- function(model){
    dev = model$null.deviance - model$deviance
    deg = model$df.null - model$df.residual
    cat('\ndeviance difference: ', dev)
    cat('\ndf difference: ', deg)
    cat('\nlevel of significance: ', pchisq(dev, deg, lower.tail = FALSE))
    cat('\nthe model is a good fit: ', pchisq(dev, deg, lower.tail = FALSE) < 0.05)
}

```

<!-- ## Comparing models
Both Full and Reduced feature sets were used for this analysis. The results obtained were:

- Full -> ROC: 0.81, TPR: 0.92, TNR: 0.36, AIC: 45.038
- Reduced -> ROC: 0.75, TPR: 0.98, TNR: 0.36, AIC: 44.661 -->

Interpreting performance metrics:

- Sensitivity (TPR) = positive results for fail dogs. How many withdrawn dogs were flaged as fail?
- Type 1 error = false positive rate: 1-TPR = 8% of the dogs flagged as fail would successed.

- Specificity (TNR) = negative result for success dogs. How many graduate dogs were flaged as success?
- Type 2 error = false negative error: 1-TNR = 64% of the dogs flagged as success would fail.
<!-- 
The results reveal that the model learnt on the reduced dataset delivers a better AUC-ROC 0.81 compared to 0.75.
On the other hand, the TPR or Sensitivity is 98% for the Reduced model and 92% for the Full model. 
Therefore, the Reduced model is a better fit for the data. 
The sample size is rather small, therefore that such differences performance may be caused for just a few dogs being correctly classified.
Regardless, a simpler model is always a better choice if the differences in performance are not significant.
Another metric that can be used to compare model is AIC (Akaike Information Criterion), an optimal model minimizes AIC.

Conclusively, the AIC indicates that the Reduced model is a better fit. -->

```{r, echo = FALSE}
print("p<0.05 + reduced")
data_prep <- impute_pca(data_reduced)
data_pca  <- bake(data_prep, data_reduced)
rfe_reduced <- rfe_lr(data_pca)
rfe_reduced
dev_null_residual(summary(rfe_reduced$fit))

# save csv and print latex code
tab_results(as.data.frame(rfe_reduced$results), name = "rfe-results",
    caption = "Recursive Feature Selection with Leave-One-Out Cross Validation"  )

# plot roc per rfe iteration
plot <- ggplot(data = rfe_reduced, metric = "ROC") + theme_bw() +
            expand_limits(y = c(0,1))
plot

fig_results(plot = plot, name = "rfe-roc",
            caption = "Recursive Feature Elimination ROC achieved considering Leave-One-Out Cross-Validation for each model versus the number of variables included in each model")
            
```

## Interpretation

The best Logistic Regression model is used to investigate the effect of the principal components on the training outcome.
The estimates were exponentiated to get the Odds Ratio (OR). The 95% Confident Interval (CI) of the ORs was also calculated. 
The OR for PC01 is 0.44, this means that the probability of withdrawal decreases by (0.44-1) 66% for every unit increase of PC1.
We are 95% confident that a unit increase in PC01 results in a 33 to 75% (0.25-1 = 0.75, 0.67-1 = 0.33) less odds of being withdrawn from training.

```{r, echo = FALSE}
results_reduced <- as.data.frame(coef(summary(rfe_reduced$fit)))
results_reduced$OR <- exp(results_reduced$Estimate)
results_reduced <- cbind(results_reduced, exp(confint(rfe_reduced$fit, level = 0.95)))
print("Results from the model created with the reduced feature set")
print(results_reduced)

# save csv and print latex code
tab_results(results_reduced,
    caption = "Best logistic regression model using selected by RFE",
    name = "rfe-lr")
```

```{r, echo = FALSE}
coef <- predictors(rfe_reduced)

perf <- as.data.frame(data_pca %>% select(all_of(coef),Outcome))
perf$Outcome <- relevel(perf$Outcome, "Fail")
perf$Probability <- predict(rfe_reduced$fit,       # model
                newdata = data_pca,        # dataframe, this will only select PC01
                type  = "response")
perf$Status <- prq$Status
perf <- perf %>% mutate(Status = Status %>% fct_relevel(c("W", "GD", "AD")))
perf$Predicted <- ifelse(perf$Probability > 0.5, "Fail", "Success")
perf$Estimate <- log( 1 / (1/perf$Probability - 1) )

# logit function, calculate y given the probability(x)
y <- function(x){ return(log(1/( 1/x - 1))) }
th_fail = 0.65
th_success = 0.35

print("Probability predicted by the LR using PRQ data vs true Outcome")
plot <- ggplot(perf, aes(x = Estimate, y = Probability, group = Outcome ))  +
    # bottom - green
    geom_rect(aes(xmin = -Inf, xmax = y(th_success), ymin = 0, ymax = th_success), 
            fill = "#99ff99", alpha = 0.2, color = "white") + 
    # top - orange
    geom_rect(aes(xmin = y(th_fail), xmax = Inf, ymin = th_fail, ymax = 1), 
            fill = "#ffff99", alpha = 0.2, color = "white") +  
    theme(legend.position = c(0.14,0.88), text = element_text(size = 20)) +
    geom_point(aes(shape=Outcome, color=Outcome))
plot

fig_results(plot = plot, name = 'logit-outcome', 
            caption = "Probability predicted by the LR using PRQ data vs true Outcome")


print("Probability predicted by the LR using PRQ data vs true Status")
# plot Logit vs Status
plot <- ggplot(perf, aes(x = Estimate, y = Probability, color = Status )) +
    # bottom - green
    geom_rect(aes(xmin = -Inf, xmax = y(th_success), ymin = 0, ymax =  th_success), 
            fill = "#99ff99", alpha = 0.2, color = "white") + 
    # top - orange
    geom_rect(aes(xmin = y(th_fail), xmax = Inf, ymin =  th_fail, ymax = 1), 
            fill = "#ffff99", alpha = 0.2, color = "white") +  
    theme(legend.position = c(0.14,0.88), text = element_text(size = 20)) +
    geom_point(aes(shape=Status, color=Status))
plot

fig_results(plot = plot, name = 'logit-status', 
            caption = "Probability predicted by the LR using PRQ data vs true Status")

# calculate accuracy
sensitivity = 0.9577    # TPR -> true fail flagged as fail
specificity = 0.3636    # TNR -> true success flagged as success 
prevalence = 11/63      # percentage of fail
accuracy = (sensitivity) * (prevalence) + (specificity) * (1 - prevalence)
cat('Accuracy:', accuracy)


print("Estimate per training outcome status")
perf %>% group_by(Status) %>% dplyr::summarize(Mean = mean(Estimate, na.rm=TRUE))

cat("Predicted probability <", th_success, " -> Green flag ")
print("Dogs flagged green are likely to SUCCEED, true outcome count and status proportion:")
table(perf %>% filter(Probability < th_success) %>% pull(Outcome))
prop.table(table(perf %>% filter(Probability < th_success) %>% pull(Status)))

cat("Predicted probability >", th_fail, "-> Yellow flag")
print("Dogs flagged green are likely to FAIL, true outcome count and status proportion:")
table(perf %>% filter(Probability > th_fail) %>% pull(Outcome))
prop.table(table(perf %>% filter(Probability > th_fail) %>% pull(Status)))
```

## Feature Importance

### Explained Variance
```{r, echo = FALSE}
# tidy(recipe steps, number = select pca step, type = coef or variance
dc_pca_var <- as.data.frame(tidy(data_prep, number = 3, type = "variance"))
dc_pca_var <- dc_pca_var %>% spread(key = "terms", value = value) %>% select(-c(id))
dc_pca_var$component <- mapvalues(dc_pca_var$component, from = 1:12, 
    to = c('PC01', 'PC02', 'PC03', 'PC04', 'PC05', 'PC06', 'PC07', 'PC08', 'PC09', 'PC10', 'PC11', 'PC12'))
colnames(dc_pca_var) <- make.names(names(dc_pca_var)) # fixing column names with space

cat("PCA features kept in the final model after RFE: ", coef)
print("Explained Variance per Principal Component (Scree Plot)")
dc_pca_var$Included = factor(c("Yes", "No", "No", "No", "No", "No", "No", "No", "No", "No", "No", "No"))
plot <- ggplot(dc_pca_var, aes(x = component, y = percent.variance)) + #, fill = Included)
    geom_bar(stat='identity', position = 'dodge', width = 0.8)+
    xlab("Principal components") +
    ylab("Explained Variance (%)") 
plot

fig_results(plot = plot, name = 'pc-explained-variance', 
            caption = "Explained variance per principal component (Scree Plot)")
```

### Coefficient Loadings

Analysis of the principal component loadings that were used in the best Multivariate Logistic Regression Model.

The 5 highest loading values in PC1 come from the variables 
"F_12 Excitability"
"F_04 Dog directed fear"
"F_09.Nonsocial.fear"
"F_10 Separation related problems"
"I_27.Toward.cats.squirrels.or.other.small.animals.entering.your.yard".
This indicates that this principal component places most variation in these variables. 

```{r, echo = FALSE}
# model prep is the recipe steps, select pca -> number = 3, because it's the 3rd step
df_pca_reduced <- as.data.frame(tidy(data_prep, number = 3, type = "coef"))
# creating another dataframe with PC1 values for the analysis
df_pcs_reduced <- df_pca_reduced  %>% filter(component == 'PC1') %>% select(terms, value) 

plot <- ggplot(df_pcs_reduced, aes(x=unlist(lapply(strsplit(terms,"[.]"), function(x) x[1])), y=value)) +
    geom_bar(stat='identity', position='dodge', width = 0.8) +
    xlab("C-BARQ items and factors - Predictor Variables") +
    ylab("Principal components") +
    theme(axis.text.x = element_text(angle = 45))
plot    

fig_results(plot = plot, name = 'pc-loading',
            caption = "PC01 coefficient loading from original items and factors")
# figuring out the righest loading items for the PC1
df_pcs_reduced %>% arrange(desc(abs(value))) %>% select(terms, value)

# save csv and print latex code
tab_results(df_pcs_reduced %>% remove_rownames %>% column_to_rownames(var="terms"), 
            name = 'pc-loading', caption = 'PC01 coefficient loading from original items and factors')
```

# Conclusion

Completing this questionnaire before training (Week 0) would allow assistance dog training organisations to learn more about the dogs starting training. 
Results from this objective assessment would assist practitioners in making informed decisions when analysing which dogs should be withdrawn for training. 
The model could be used to flag dogs likely to be withdrawn from training for behavioural reasons. 
Dog trainers could consider the output to re-assess and adjust the training programme for flagged dogs in order to address behavioural issues.
Considering the higher TPR achieved by the Reduced model, that might be a better choice when employing this system to correcly identify unsuitable dogs for the training programme.

