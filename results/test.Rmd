# Correlation
Correlation matrix and heatmap for all the variables in the Dog Trainer Questionnaire. 
Items within the same canine personality subscale typically have a higher correlation.

```{r, echo = FALSE}
# OPTION 1
# calculate correlation matrix
correlationMatrix <- round(cor(data[,1:26]),2)
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.6, names = TRUE, exact = TRUE)
# print indexes of highly correlated attributes
print(highlyCorrelated)
ggplot(data = melt(correlationMatrix), aes(x=Var1, y=Var2,
                                   fill=value)) + geom_tile()
write.table(correlationMatrix, file = "models/R/dtq-correlation.csv")
```
```{r, echo = FALSE}
# OPTION 2
M<-abs(cor(training[,-58]))#left 58 column on the training dataset because that is going to be the output variable. We are checking correlation among all other variables in the training dataset, and we are taking the absolute value.
diag(M)<-0 #setting the correlation of a variable to itself to be a zero. Because we know that every variable has a correlation of 1 with itself
which(M>0.8, arr.ind=T)#Checking which of the variables have higher correlation than 0.8. 
```

# Model
```{r, echo = FALSE}
# I think this is not valid because for binary data, only the difference in deviance follows a chisq distribution
dev_critical <- function(model){
    critical = qchisq(0.95, model$df.residual, lower.tail = TRUE)
    residual = model$deviance
    cat('critical deviance: ', critical, '\nresidual deviance: ', residual)
    cat('\nmodel is a good fit: ', residual < critical)
}
# dev_critical(model)
```

# Univariate Logistic Regression
```{r, echo = FALSE}
# option 2 for univariate logistic regression using one predictor and outcome
# I didn't use this as I preferred using a two step approach so I could have
# 1 variable = list of model results output for each feature
# 1 variable = dataframe with the model results pulled (p_value and deviance)
lr_test <- lapply(data[-length(data)], function(x){
    model <- summary(glm(formula = Outcome ~ x, data = data, family = binomial(link = logit), na.action = na.exclude))
    c(coef(model)), model$deviance) })
lr_test
```



## Chisq for the univariate analysis

Using Chisq test on contingency tables created with each feature and the outcome.

```{r, echo = FALSE}
# chisq test between predictors and outcome on contingency tables
chi_test <- lapply(data[-length(data)], function(x) chisq.test(table(x, data$Outcome)))
chi_results <- data.frame(do.call(rbind, lapply(chi_test, function(x) c( x$parameter, x$statistic, round(x$p.value,4)))))
colnames(chi_results)[3] <- "p_value"
chi_results$significant_0.05 <- (chi_results$p_value < 0.05)
chi_results$significant_0.1 <- (chi_results$p_value < 0.1)
chi_results$significant_0.2 <- (chi_results$p_value < 0.2)
chi_results[order(chi_results$p_value),]
print("Number of variables considering each level of significance using Chisq test")
print(colSums(chi_results[,4:6], na.rm = TRUE))
print(chi_results[chi_results$significant_0.05 == TRUE, 'p_value', drop = FALSE])

feat_chi = rownames(chi_results[chi_results$p_value < 0.2,])
data_chisq <- data %>% select(all_of(feat_chi), Outcome)
```

Checking the intersection and difference between chisq and logistic regression 
```{r, echo = FALSE}
cat("Chisq p_value < 0.2\n", feat_chi)
cat("Logistic Regression p_value < 0.2\n", feat_lr)
cat("intersection lr and chi:\n", intersect(feat_lr, feat_chi))
cat("difference lr - chi:\n", setdiff(feat_lr, feat_chi))
cat("difference chi - lr:\n", setdiff(feat_chi, feat_lr))
```

Considering features selected with univariate chisq test pvalue.
```{r, echo = FALSE}
# Recursive Feature Elimination with feat_chisq
rfe_chi <- rfe(Outcome ~ . ,                 # predict Outcome using all other variables
            data = data_chisq,              # selecting the features from chisq
                   sizes = c(1:length(data_chisq)-1),
                   rfeControl = ctrl,
                   metric = "ROC")
                
warnings()   
print(rfe_chi)
ggplot(data = rfe_chi, metric = "ROC") + theme_bw()
cat("Features in the model that delivered the best ROC", predictors(rfe_chi))

model_chi <- rfe_chi$fit
summary(model_chi)

### Principal Component Analysis (PCA)
Principal component analysis was used to combine features that showed high multicollinearity.
VIF = 1 no correlation, 5 < VIF < 10 moderate correlation, VIF >10 high multicollinearity. 
All variables from both the full and reduced features sets had VIF > 5, therefore PCA was performed.
PCA can be implemented using prcomp function, however, it deletes all the row with NAs.
Therefore PCA will be implemented in the RFE cross validation loop as a preprocessing method after missing value imputation.

## Recursive Feature Elimination (RFE) + Multivariate Logistic Regression (LR)

```{r, echo = FALSE}
# simple model used for dtq without performing PCA 
feat_lr = lr_results %>% filter(significant_0.05 == TRUE) %>% rownames()
data_lr <- data %>% select(all_of(feat_lr), Outcome)

# Setting ROC as the metric for the Logistic Regression function
lrFuncs$summary <- twoClassSummary

ctrl <- rfeControl(functions = lrFuncs,     # logistic regression
                    method = "LOOCV",       # Leave One Out Cross Validation
                    verbose = FALSE)

# Recursive Feature Elimination with feat_lr
model <- rfe(Outcome ~ .,      # predict Outcome using all other variables
                data = data_lr,     # selecting the features from univariate lr
                   sizes = c(1:length(data_lr)-1),
                   rfeControl = ctrl,
                   metric = "ROC")

warnings()
print(model)
summary(model$fit)
ggplot(data = model, metric = "ROC") + theme_bw()
cat("Features in the model that delivered the best ROC", predictors(model))
```


Conclusion
changing size variable, i.e. adding more features to the RFE does not yield a better model, as it will choose a small set



## Feature Importance
### Full Feature Set
The 5 highest values in PC1 are 
"X26 Toward unfamiliar dogs visiting your home", 
"X45 When approached directly by an unfamiliar dog of the same or larger size",
"X29 When barked growled or lunged at by another unfamiliar dog", 
"F_03 Dog directed aggression", 
"F_04 Dog directed fear"
which indicate that this principal component places most variation in these variables related to fearful and agressive behaviour.

The 5 highest values in PC5 are
"X75 Chases or would chase birds given the opportunity"
"X77 Escapes or would escape from home or yard given the chance"
"X99 Licks people or objects excessively"
"X82 Begs persistently for food when people are eating"
"F_09 Nonsocial fear"
which indicate that this principal component places most variation in these variables. 
(chasing, escaping, licking, food drive, nonsocial fear)

```{r, echo = FALSE}
# model prep is th receipe steps, for the pca, number = 3, because it's the 3rd step
df_pca_full <- as.data.frame(tidy(data_0.05_full_prep, number = 3))
# creating another dataframe with PC1 and PC5 values for the analysis
df_pcs_full <- df_pca_full %>% filter(component == 'PC1' | component == 'PC5') %>% select(terms, value, component)

# figuring out the righest loading items for the PC1 and PC5 
print("PC1 and PC2 values")
df_pcs_full %>% filter(component == 'PC1') %>% arrange(desc(abs(value))) %>% select(terms, value)
df_pcs_full %>% filter(component == 'PC5') %>% arrange(desc(abs(value))) %>% select(terms, value)

ggplot(df_pcs_full, aes(x=unlist(lapply(strsplit(terms,"[.]"), function(x) x[1])), y=value, fill=component)) +
    geom_bar(stat='identity', position='dodge', width = 0.8) +
    xlab("C-BARQ items or factors") +
    ylab("Principal components") +
    theme(axis.text.x = element_text(angle = 45), legend.position = "top")
print(df_pcs_full$terms[1:29])
```



## Model Analysis

The best Logistic Regression model is used to investigate the effect of each variable on the training outcome.
Odds ratio calculation, consider using Bon Ferroni correction

```{r definition, echo = FALSE}
dev_null_residual <- function(model){
    dev = model$null.deviance - model$deviance
    df = model$df.null - model$df.residual
    cat('\ndeviance difference: ', dev)
    cat('\ndf difference: ', df)
    cat('\nlevel of significance: ', pchisq(dev, df, lower.tail = FALSE))
    cat('\nmodel is a good fit: ', pchisq(dev, df, lower.tail = FALSE) < 0.05)
}
```

```{r, echo = FALSE}
print('Best model summary')
print(summary(model$fit))

print('Best model fit analysis (difference in deviance considering the degrees of freedom')
dev_null_residual(summary(model$fit))

# exponentiate the coefficentes to get the odd ratio and their confidence intervals
## odds ratios and 95% CI using profiled log-likelihood
round(exp(cbind(OR = coef(model$fit), confint(model$fit, level = 0.95))),4)
```
After exponentiating the Odds Ratio and 95% CI 
            OR      2.5%    97.5%
(Intercept) 0.1327 0.0458 0.2988
PC01        0.6167 0.4223 0.7976
PC05        0.6057 0.3100 1.1220


## Feature Importance
Principal Component Analysis output. Considering that the best model chose 

```{r, echo = FALSE}
model_pca <- recipe(formula =  ~ ., data = data_0.05_full) %>%
                step_normalize(all_numeric()) %>%
                # step_impute_mean(all_numeric()) %>%
                step_impute_knn(all_numeric(), neighbors = 3) %>%
                step_pca(all_numeric(), threshold = .95)

model_prep <- prep(model_pca, training = data_0.05_full)

# model prep is th receipe steps, for the pca, number = 3, because it's the 3rd step
df_pca <- as.data.frame(tidy(model_prep, number = 3))
df_pc1 <- df_pca %>% filter(component == 'PC1') %>% select(terms,value)
df_pc5 <- df_pca %>% filter(component == 'PC5') %>% select(value)
df_pcs <- cbind(df_pc1, df_pc5)
colnames(df_pcs)[2:3] <- c('PC1', 'PC5')
colnames(df_pca)
ggplot(aes(x = )) + geom_col

df_pcs <- df_pca %>% filter(component == 'PC1' | component == 'PC5') %>% select(terms, value, component)

ggplot(df_pcs, aes(x=unlist(lapply(strsplit(terms,"[.]"), function(x) x[1])), y=value, fill=component)) +
    geom_bar(stat='identity', position='dodge', width = 0.8) +
    xlab("C-BARQ items or factors") +
    ylab("Principal components") +
    theme(axis.text.x = element_text(angle = 45), legend.position = "top")
```

## Diagnostics
1. Index plot: residuals vs case number
2. Leverage cases
3. Influence cases

```{r, echo = FALSE}
diagnostics <- function(model){
    h <- lm.influence(model)$hat
    rpear <- residuals(model, "pearson")/sqrt(1-h)
    rdev <- residuals(model, "deviance")/sqrt(1-h)
    phat <- model$fitted.values
    D <- rpear*rpear*h/(2**(1-h))
    diagnostic <- data.frame(data$Outcome, phat, h, rpear, rdev, D)
    return(diagnostic)

    plot(rpear, main="Index Plot of Pearson Residuals")
    plot(model$linear.predictors, rpear, main = "Plot of Pearson Residuals vs Linear Predictor")
}

diag_lr <- diagnostics(model_lr)
diag_lr[order(diag_lr$rpear, decreasing = TRUE),]
```



## RFE issues posted on github 30/08/2022

```{r, echo = FALSE}
library(caret)
library(dplyr)
set.seed(1)
dat <- twoClassSim(100)
print(levels(dat$Class)) # Class 1 and Class2

# Setting PR/ROC as the metric for the Logistic Regression function
lrFuncs$summary <- prSummary # twoClassSummary 

ctrl <- rfeControl(functions = lrFuncs,         # Logistic Regression
                # method = 'LOOCV',             # error here -> predictions are not availabel (model_rfe$pred = NULL) when using LOOCV 
                method = "cv",                  # Using Cross Validation with
                number = 100,                   # Number of folds = number of samples
                saveDetails = TRUE,
                returnResamp = "all",
                rerank = TRUE,
                verbose = FALSE)

# Recursive Feature Elimination with feat_lr
model_rfe <- rfe(Class ~ .,                     # predict Class using all other variables
                data = dat,
                sizes = 1:5,                    # from 1 to 5 variables
                rfeControl = ctrl,
                # metric = "AUC",               # cv with folds = 100 does not calculate AUC/ROC on entire cross validated set
                metric = "Recall",              # optimising Recall
                maximize = TRUE)

print(model_rfe)
colnames(model_rfe$pred)
# model predicts 'Class 2'
print(model_rfe$fit)
model_rfe_pred <- model_rfe$pred %>% filter(Variables == 1) %>% select(pred, obs)
# calculating the confusion matrix using Class 2 as positive class does not give me the same results
confusionMatrix(data = model_rfe_pred$pred, reference = model_rfe_pred$obs, positive = "Class2",  mode = "prec_recall")
# however, I get the same results if I swap the positive class to 'Class 1'
confusionMatrix(data = model_rfe_pred$pred, reference = model_rfe_pred$obs, positive = "Class1",  mode = "prec_recall")

# checking the conclusion above
prob <- predict(model_rfe$fit,       # model
                newdata = dat %>% select(-Class),
                type  = "response")
model_pred <- data.frame("prob" = prob, "obs" = dat$Class)
# if probability > 0.5 -> pred1 predicts Class 1 and pred2 predicts Class 2
model_pred <- model_pred %>% mutate(pred1 = factor(if_else(prob > 0.5, "Class1", "Class2") ))
model_pred <- model_pred %>% mutate(pred2 = factor(if_else(prob > 0.5, "Class2", "Class1") ))

confusionMatrix(data = model_pred$pred1, reference = model_pred$obs, positive = "Class1",  mode = "prec_recall")
confusionMatrix(data = model_pred$pred2, reference = model_pred$obs, positive = "Class1",  mode = "prec_recall")

model_glm <- glm(Class ~ Linear03, 
        data=dat, family = binomial(link = logit))

confusionMatrix(data = factor(round(pred)), reference = factor(as.numeric(dat$Class)-1), positive = "1",  mode = "prec_recall")
confusionMatrix(data = factor(round(pred)), reference = dat$Class, positive = "Class1")


```


Example from the internet
```{r, echo = FALSE}
data(PimaIndiansDiabetes)
require(caret)
require(pROC)

rfFuncs$summary <- twoClassSummary
ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   number = 10,
                   repeats = 3,
                   saveDetails = TRUE,
                   returnResamp = "all")

trainctrl <- trainControl(classProbs= TRUE,
                          verboseIter = TRUE,
                          summaryFunction = twoClassSummary, 
                          method = "cv", 
                          number = 10 ,
                          returnResamp = "final", 
                          returnData = TRUE)
set.seed(12)
tunegrid <- expand.grid(.mtry=c(1:10))
rfe_rf <- rfe(PimaIndiansDiabetes[,1:8], PimaIndiansDiabetes[,9], sizes=c(1:8),
           method="rf",
           rfeControl = ctrl, 
           metric = "ROC", 
           trControl = trainctrl,
           tuneGrid = tunegrid,
           preProc = c("center", "scale"))
rfe_rf
selectedIndices <- rfe_rf$pred$Variables == rfe_rf$optsize
require(pROC)
table(rfe_rf$pred$obs)
ROC = plot.roc(rfe_rf$pred$obs[selectedIndices],rfe_rf$pred$neg[selectedIndices], )
ROC$auc
# predicting positives
levels(PimaIndiansDiabetes[,9])
colnames(rfe_rf$pred)

df_perf <- rfe_rf$pred %>% filter(Variables == 8) %>% select(pos, obs)

levels(rfe_rf$pred$obs)

roc.curve(scores.class0 = df_perf$pos, weights.class0 = abs(as.numeric(df_perf$obs)-1), curve = TRUE)
ROC$auc
```